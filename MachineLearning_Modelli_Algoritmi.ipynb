{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wS0Ta35Wx9P8pmEW8xpuDv1VzB9mQZpm",
      "authorship_tag": "ABX9TyNncSTGQ8+1l9tsMwcFK9pG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoRigoni/MachineLearningPlayground/blob/master/MachineLearning_Modelli_Algoritmi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent (teoria)"
      ],
      "metadata": {
        "id": "RSWph7IJuiwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "x4DYXzDXukHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creazione di un dataset di esempio e divisione training/test...\n",
        "from sklearn.datasets import make_regression\n",
        "x, y = make_regression(n_samples=100, n_features=50, bias=5., noise=20., random_state=RANDOM_SEED)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "XmsNelsxu_-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor #stocastics, se vogliamo full mettiamo batch_size uguale ampiezza dataset\n",
        "\n",
        "# se si ottiene \"ConvergenceWarning\" indica che abbiamo finito le epoche prima della conservenza.\n",
        "model = SGDRegressor(max_iter=1000)\n",
        "model.fit(x_train, y_train) #addestramento"
      ],
      "metadata": {
        "id": "fQcw7nLMvhYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#funzione di aiuto valutazione modello\n",
        "def evaluate(model, data):\n",
        "  x,y = data\n",
        "  y_pred = model.predict(x)\n",
        "  print(f\"MSE={mean_squared_error(y, y_pred)}\")\n",
        "  print(f\"R2={r2_score(y, y_pred)}\")"
      ],
      "metadata": {
        "id": "bdN3TXYyv-O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valutiamo il modello...\n",
        "evaluate(model, (x_train, y_train))\n",
        "evaluate(model, (x_test, y_test))\n",
        "\n",
        "#R molto alto, potrebbe esserci overfitting, infatti MSE molto alto nel test rispetto al train\n",
        "#risciviamo con penalizzazione, usando L1+L2 pesate=elasticnet\n",
        "#  usiamo learning rate adattivo, che varia durante le varie epoche, quindi gli step finale saranno più piccoli e precisi\n",
        "#  aumentiamo max_iter per non avere più warning\n",
        "model = SGDRegressor(max_iter=5000, penalty=\"elasticnet\", alpha=0.01, l1_ratio=0.9, learning_rate=\"adaptive\")\n",
        "\n",
        "model.fit(x_train, y_train) #addestramento\n",
        "\n",
        "#rivalutiamo il modello...\n",
        "evaluate(model, (x_train, y_train))\n",
        "evaluate(model, (x_test, y_test))\n",
        "#c'è ancora overfitting in parte, ma se non è cambiato nulla, vuol dire che abbiamo già raggiunto migliori performance..."
      ],
      "metadata": {
        "id": "_F079386wW_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modello di classificazione con Stocasting Descent\n",
        "\n",
        "Learning rate puà avere come valori: constant con valore di eta0, optimal, invscaling di default"
      ],
      "metadata": {
        "id": "tc86ut-jzYiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "#dataset di esempio\n",
        "x, y = make_classification(n_samples=100, n_features=30, n_informative=30, n_redundant=0, n_repeated=0, n_classes=2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "nlyDxOJszeLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#funzione di valutazione\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def evaluate(model, data):\n",
        "  x,y = data\n",
        "  y_pred = model.predict(x)\n",
        "  print(f\"Accuracy={accuracy_score(y, y_pred)}\")\n",
        "  print(f\"Log Loss={log_loss(y, y_pred)}\")"
      ],
      "metadata": {
        "id": "oNy9occq0Jr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier # simile a quanto fatto con regressione lineare..\n",
        "\n",
        "#divisione dataset train, test\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED)\n",
        "\n",
        "model = SGDClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "evaluate(model, (x_train, y_train))\n",
        "evaluate(model, (x_test, y_test))\n",
        "\n",
        "#sembra esserci overfitting, regolizziamo...\n",
        "model = SGDClassifier(penalty=\"elasticnet\", alpha=0.01, l1_ratio=0.8, learning_rate=\"constant\", eta0=10)\n",
        "model.fit(x_train, y_train)\n",
        "evaluate(model, (x_train, y_train))\n",
        "evaluate(model, (x_test, y_test))\n",
        "#peggiora, quindi lasciamo così\n",
        "\n",
        "model = SGDClassifier(learning_rate=\"constant\", eta0=10) # se troppo alto il modello rischia di rimbalzare, senza trovare il minimo\n",
        "model.fit(x_train, y_train)\n",
        "evaluate(model, (x_train, y_train))\n",
        "evaluate(model, (x_test, y_test))"
      ],
      "metadata": {
        "id": "ZD9PbRfH0bet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mini batch Gradient Descent\n",
        "\n",
        "Di default sklearn analizza un esempio per volta, col il mini, possiamo lavorare a porzioni di dati"
      ],
      "metadata": {
        "id": "wAhb5J6519u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "#dataset di esempio\n",
        "x, y = make_classification(n_samples=100, n_features=30, n_informative=30, n_redundant=0, n_repeated=0, n_classes=2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "00AkD8BI21jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#epochs = 5\n",
        "#epochs = 350 #full\n",
        "epochs = 70 #mini batch\n",
        "#n_batches = x_train.shape[0] # facendo così abbiamo lo stocatics\n",
        "#n_batches = 1 #full\n",
        "n_batches = 12\n",
        "\n",
        "batch_size = x_train.shape[0]/n_batches\n",
        "\n",
        "classes = np.unique(y_train) #abbiamo due classi essendo binaria\n",
        "\n",
        "sgd = SGDClassifier(loss=\"log\") #funzione di costo log loss\n",
        "sgd_loss = []\n",
        "\n",
        "tick = time()\n",
        "\n",
        "for epoch in range(epochs): #alla fine di ogni epoca dobbiamo mischiare i dati\n",
        "  x_shuffled, y_shuffled = shuffle(x_train, y_train)\n",
        "  for batch in range(n_batches):\n",
        "    batch_start = int(batch*batch_size)\n",
        "    batch_end = int((batch+1)*batch_size)\n",
        "\n",
        "    x_batch = x_shuffled[batch_start:batch_end, :]\n",
        "    y_batch = y_shuffled[batch_start:batch_end]\n",
        "\n",
        "    #step di epoca di addestramento sul batch\n",
        "    sgd.partial_fit(x_batch, y_batch, classes=classes) #vanno passate classi perchè non è detto che ci siano tutte nel batch\n",
        "    #calcoliamo log loss...\n",
        "    loss = log_loss(y_train, sgd.predict_proba(x_train), labels=classes)\n",
        "    sgd_loss.append(loss)\n",
        "\n",
        "  print(f\"Loss all'epoca {epoch+1} = {loss}\")\n",
        "\n",
        "print(f\"Addestramento completato in {time()-tick:.2f} secondi\")"
      ],
      "metadata": {
        "id": "A0lGNSwP3Bjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mostriamo risultato graficamente...\n",
        "def train_history(losses, title):\n",
        "  plt.figure(figsize=(14,10))\n",
        "  plt.title(title)\n",
        "  plt.xlabel(\"Iterazione\") #che non è l'epoca, ma ogni step, con x_train=7 e epoch=5, saranno 350 iterazioni\n",
        "  plt.ylabel(\"Log-Loss\")\n",
        "  plt.plot(losses)\n",
        "\n",
        "train_history(sgd_loss, \"Stochasting Gradient Descend\")\n",
        "\n",
        "#c'è rumore nella fase di addestramento, si riprova con 350 epoche e batch=1\n",
        "#dopo 100 epoche modello ha raggiunto convergenza e rimane lineare\n",
        "#poi proviamo con batch size di 12 (mini batch) e numero epoche 70, così via di mezzo, c'è un pò di rumore, ma modello migliora linearmente"
      ],
      "metadata": {
        "id": "ETINASY76HUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Early stopping\n",
        "\n",
        "Tecnica che permette di stoppare il modello se ha smesso di migliorare, ovvero di diminuire dopo tot epoche\n",
        "\n",
        "Esiste anche come parametro in SGDRegression"
      ],
      "metadata": {
        "id": "hnYJWje98AoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#refactoring del codice precedente:\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "epochs = 70\n",
        "n_batches = 12\n",
        "\n",
        "#se dopo 5 iterazioni, il costo non è migliorato di almento 0.0001 stoppiamo addestramento\n",
        "tol = 0.0001\n",
        "n_iter_no_change = 5\n",
        "n_iter_count = 0\n",
        "best_loss = 100 #valore migliore fino a quel momento\n",
        "\n",
        "batch_size = x_train.shape[0]/n_batches\n",
        "\n",
        "classes = np.unique(y_train) #abbiamo due classi essendo binaria\n",
        "\n",
        "sgd = SGDClassifier(loss=\"log\") #funzione di costo log loss\n",
        "sgd_loss = []\n",
        "\n",
        "tick = time()\n",
        "\n",
        "for epoch in range(epochs): #alla fine di ogni epoca dobbiamo mischiare i dati\n",
        "  x_shuffled, y_shuffled = shuffle(x_train, y_train)\n",
        "  for batch in range(n_batches):\n",
        "    batch_start = int(batch*batch_size)\n",
        "    batch_end = int((batch+1)*batch_size)\n",
        "\n",
        "    x_batch = x_shuffled[batch_start:batch_end, :]\n",
        "    y_batch = y_shuffled[batch_start:batch_end]\n",
        "\n",
        "    #step di epoca di addestramento sul batch\n",
        "    sgd.partial_fit(x_batch, y_batch, classes=classes) #vanno passate classi perchè non è detto che ci siano tutte nel batch\n",
        "    #calcoliamo log loss...\n",
        "    loss = log_loss(y_train, sgd.predict_proba(x_train), labels=classes)\n",
        "    sgd_loss.append(loss)\n",
        "\n",
        "  #valutazione alla fine di ogni epoca...\n",
        "  if loss >= best_loss-tol:\n",
        "    if n_iter_count >= n_iter_no_change:\n",
        "      print(\"Early Stopping!!!\")\n",
        "      break\n",
        "    else:\n",
        "      n_iter_count+=1\n",
        "  else: # nuovo valore migliore..\n",
        "    best_loss= loss\n",
        "    n_iter_count=0\n",
        "\n",
        "  print(f\"Loss all'epoca {epoch+1} = {loss} ({best_loss-loss})\")\n",
        "\n",
        "print(f\"Addestramento completato in {time()-tick:.2f} secondi\")"
      ],
      "metadata": {
        "id": "7qw0Y5bE8Hx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gradient Descent (esercitazione)"
      ],
      "metadata": {
        "id": "1x2rASvi9YXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model  import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics  import log_loss\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, accuracy_score, log_loss"
      ],
      "metadata": {
        "id": "3-h9kCns9Y9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 0\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-modelli-e-algoritmi/main/datasets/\""
      ],
      "metadata": {
        "id": "j-xRROarIevR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#diagnosis è la variabile target che indica se il tumore è maligno/benigno\n",
        "#sappiamo già che il dataset non ha difetti e saltiamo la fase di analisi dei dati\n",
        "\n",
        "df = pd.read_csv(BASE_URL + \"breast_cancer.csv\")\n",
        "df.count()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "r3DegqUpIx9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_dict = {\"M\":1, \"B\":0}\n",
        "df[\"diagnosis\"] = df[\"diagnosis\"].map(lambda x: map_dict[x])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "pxaRh1iSJamN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparazione array di valutazione\n",
        "x= df.drop([\"diagnosis\", \"ID number\"], axis=1).values\n",
        "y = df[\"diagnosis\"].values\n",
        "\n",
        "#hold out\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED)\n",
        "\n",
        "#riportiamo variabili su stessa scala...\n",
        "ss = StandardScaler()\n",
        "x_train = ss.fit_transform(x_train)\n",
        "x_test = ss.transform(x_test)"
      ],
      "metadata": {
        "id": "RtXoQlATJqAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fase di addestramento\n",
        "def fit(model, x, y, batch_size=1, epochs=100, verbose=True): # default batch-size=1, quindi stochatics  descend\n",
        "  n_batches = int(x.shape[0]/batch_size)+1\n",
        "  classes = np.unique(y) # potrebbero mancare classi in quel batch\n",
        "\n",
        "  batch_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    x_shuffled, y_shuffled = shuffle(x, y)\n",
        "    for batch in range(n_batches):\n",
        "      batch_start = int(batch*batch_size)\n",
        "      batch_end = int((batch+1)*batch_size)\n",
        "      x_batch = x_shuffled[batch_start:batch_end, :]\n",
        "      y_batch = y_shuffled[batch_start:batch_end]\n",
        "      model.partial_fit(x_batch, y_batch, classes = classes)\n",
        "      loss = log_loss(y_test, model.predict_proba(x_test), labels=classes)\n",
        "      batch_loss.append(loss)\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Loss all'epoca {epoch+1} ) {loss}\")\n",
        "\n",
        "  return model\n",
        "\n",
        "def evaluate(model, x, y, label=None):\n",
        "  y_pred = model.predict(x)\n",
        "  y_proba = model.predict_proba(x)\n",
        "\n",
        "  if label is not None:\n",
        "    print(label)\n",
        "\n",
        "  accuracy = accuracy_score(y, y_pred)\n",
        "  loss = log_loss(y, y_proba)\n",
        "\n",
        "  print(f\"Accuracy={accuracy} Log Loss = {loss:.3f}\")\n",
        "  return accuracy, loss"
      ],
      "metadata": {
        "id": "N6vU-U2fKSdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#addrestiamo con differenti batch_size...\n",
        "batch_sizes = [8, 16, 32, 64, 128]\n",
        "\n",
        "best_model = None\n",
        "best_loss = 100\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "  print(f\"Batch size = {batch_size}\")\n",
        "  sgd = SGDClassifier(loss=\"log_loss\")\n",
        "  fit(sgd, x_train, y_train, batch_size=batch_size, epochs=200, verbose=False)\n",
        "  _, loss = evaluate(sgd, x_test, y_test, label=\"TEST SET\")\n",
        "\n",
        "  if best_model is None or loss < best_loss:\n",
        "    best_model = sgd\n",
        "    best_loss = loss\n",
        "\n",
        "  #consideriamo migliore accuracy e loss più basso\n",
        "\n",
        "print(classification_report(y_train, best_model.predict(x_train)))\n",
        "print(classification_report(y_test, best_model.predict(x_test))) #vediamo se mantiene performance sul set di test:Sì\n"
      ],
      "metadata": {
        "id": "JOUVuOzdMfhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valutazione di un nuovo dataset che è stato fornito\n",
        "df_update = pd.read_csv(BASE_URL+\"breast_cancer_update.csv\")\n",
        "df_update[\"diagnosis\"] = df_update[\"diagnosis\"].map(lambda x:map_dict[x])\n",
        "\n",
        "x_update = df.drop([\"diagnosis\", \"ID number\"], axis=1).values\n",
        "y_update = df[\"diagnosis\"].values\n",
        "\n",
        "#standardizzazione con lo stesso modello usato durante l'addestramento\n",
        "#(in un caso reale avremmo dovuto esportare il precedente modello e reimportarlo in un nuovo notebbok)\n",
        "x_update = ss.transform(x_update)\n",
        "best_model.partial_fit(x_update, y_update)\n",
        "\n",
        "#come metriche non è cambiato nulla\n",
        "print(classification_report(y_test, best_model.predict(x_test)))\n"
      ],
      "metadata": {
        "id": "WntACQ3LJTVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes (teoria)"
      ],
      "metadata": {
        "id": "IyJUSn3iUHdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gaussian Naive Bayes\n",
        "\n",
        "Assume che la distrubuizione della probabilità x dato y, sia gaussiana (con maggioranza attorno la media)\n"
      ],
      "metadata": {
        "id": "dUu_qZg5UKND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris, make_gaussian_quantiles, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Xkxop84Cb6JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "gAChPFe_cCHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_gaussian_quantiles(n_features=1, n_classes=2, random_state=RANDOM_SEED) #dataset di prova\n",
        "plt.scatter(np.arange(x.shape[0]), x, c=y) # si vede distribuzione attorno alla media che è zero"
      ],
      "metadata": {
        "id": "Sl8LynW0cERR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proviamo prima semplice algoritmo regressione logistica per la classdificazione\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED) # non serve standardizzare, lo è per definizione di gaussiana\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "lr.score(x_test, y_test) #accuracy di 0.37, estremamente basso\n",
        "\n",
        "a = np.arange(x.shape[0])\n",
        "plt.scatter(a, x, c=y)\n",
        "plt.plot(a, a*lr.coef_[0] + lr.intercept_) # mostriamo anche retta di intercetta\n",
        "\n",
        "#un sempice modello lineare non sarebbe mai potuto andare meglio di un 50%, in quanto una retta non puà classificare bene questo dataset, dato che segue una gaussiana"
      ],
      "metadata": {
        "id": "L_wvzFzqc0vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proviamo quindi altro..\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train)\n",
        "gnb.score(x_test, y_test) #accuracy del 0.93 sul test\n",
        "y_proba = gnb.predict_proba(x_test)\n",
        "log_loss(y_test, y_proba) #0.19, piccola quindi il modello si comporta bene"
      ],
      "metadata": {
        "id": "rlk7NnrveEFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bernoulli\n",
        "\n",
        "Da usare per feature binarie, ad esempio sui testi, bag of words, creiamo vocabolario (un array) con tutte le parole. Poi per ogni parola inseriamo un 1 nell'array creato alla posizione corrispondente.\n",
        "\n",
        "Esempio: riconiscimento sms di spam\n"
      ],
      "metadata": {
        "id": "c3VXXgdRfIBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris, make_gaussian_quantiles, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fKy1KEs6fJRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 0\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-modelli-e-algoritmi/main/datasets/spam.csv\""
      ],
      "metadata": {
        "id": "4DF8T1e7gRL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL)\n",
        "df.head() #1° colonna è un id, non ci importa"
      ],
      "metadata": {
        "id": "_30HAl0Fg2Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#costruzione del vocabolario per il bag of words\n",
        "def build_vocab(corpus):\n",
        "  vocab = []\n",
        "  for doc in corpus:\n",
        "    for word in doc.split():\n",
        "      if word not in vocab:\n",
        "        vocab.append(word.lower())\n",
        "\n",
        "  return vocab\n",
        "\n",
        "# i cicli in python sono molto efficienti, proviamo strada alternativa...\n",
        "def build_vocab_opt(corpus):\n",
        "  vocab = set({})\n",
        "  for doc in corpus:\n",
        "    vocab = vocab.union(set(doc.lower().split()))\n",
        "  return list(vocab)\n",
        "\n",
        "corpus = [\"Il cane gioca con la palla\", \"Il sole brilla nel cielo\"]\n",
        "build_vocab(corpus)\n",
        "build_vocab_opt(corpus) #ordine diverso ma non conta"
      ],
      "metadata": {
        "id": "_OGrBdsOhDLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_bow(corpus, vocab=None):\n",
        "  if vocab is None:\n",
        "    vocab = build_vocab(corpus)\n",
        "\n",
        "  vocab_size = len(vocab)\n",
        "  docs_bow = []\n",
        "\n",
        "  for doc in corpus:\n",
        "    doc_bow = [0]*vocab_size # inizializziamo un array con tutti zero\n",
        "    for i in range(vocab_size): # per ogni token del vocabolario...\n",
        "      doc_bow[i] = int(vocab[i] in doc)\n",
        "    docs_bow.append(doc_bow)\n",
        "\n",
        "  return docs_bow\n",
        "\n",
        "#esempio\n",
        "bow = binary_bow(corpus)\n",
        "bow"
      ],
      "metadata": {
        "id": "A7kjvANPiWI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms_list = df[\"MESSAGE\"].to_list()\n",
        "sms_list[:5]\n",
        "\n",
        "sms_bow = binary_bow(sms_list)\n",
        "len(sms_bow[0])\n",
        "\n",
        "x = sms_bow\n",
        "y= df[\"SPAM\"]"
      ],
      "metadata": {
        "id": "mfwQSB_xjpiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n"
      ],
      "metadata": {
        "id": "5w6wNPCpkL0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "benb = BernoulliNB()\n",
        "benb.fit(x_train, y_train)\n",
        "report = classification_report(benb.predict(x_test), y_test)\n",
        "print(report) #f1-score che è cumulativo degli altri, molto alto, bene"
      ],
      "metadata": {
        "id": "tKXrZmaZkVXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Multinomial Naive Bayes\n",
        "\n",
        "Rispetto Bernoulli qui usiamo bow originale, dove non si indica solo presenza parola, ma anche il conteggio, quindi usiamo versione multinomiale, con iperparametro alpha"
      ],
      "metadata": {
        "id": "Cfhk7ZTglVoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 0\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-modelli-e-algoritmi/main/datasets/spam.csv\"\n",
        "\n",
        "def build_vocab_opt(corpus):\n",
        "  vocab = set({})\n",
        "  for doc in corpus:\n",
        "    vocab = vocab.union(set(doc.lower().split()))\n",
        "  return list(vocab)\n",
        "\n",
        "def bow(corpus, vocab=None):\n",
        "  if vocab is None:\n",
        "    vocab = build_vocab_opt(corpus)\n",
        "\n",
        "  vocab_size = len(vocab)\n",
        "  docs_bow = []\n",
        "\n",
        "  for doc in corpus:\n",
        "    doc_bow = [0]*vocab_size # inizializziamo un array con tutti zero\n",
        "    for i in range(vocab_size): # per ogni token del vocabolario...\n",
        "      doc_bow[i] = doc.split().count(vocab[i])  #dobbiamo inserire quante volte la parola è presente, non solo se c'è o meno come nel precedente \"binario\"\n",
        "    docs_bow.append(doc_bow)\n",
        "\n",
        "  return docs_bow\n",
        "\n",
        "corpus = [\"Il cane gioca con la palla\", \"Il sole brilla nel cielo con la palla\"]\n",
        "bow(corpus)"
      ],
      "metadata": {
        "id": "hqrBxlmolXwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sms_bow = bow(sms_list)\n",
        "\n",
        "x = sms_bow\n",
        "y = df[\"SPAM\"].to_list()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train, y_train)\n",
        "print(classification_report(y_test, mnb.predict(x_test))) #migliore su precision e recall rispetto a Bernoulli, ma simili\n",
        "\n",
        "#ma se usassimo Bernoulli su dataset non binario?\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(x_train, y_train)\n",
        "print(classification_report(y_test, bnb.predict(x_test)))\n",
        "\n",
        "#se i dati non sono binari, Bernoulli li \"binarizza\", quindi otteniamo stesso risultato\n",
        "#normliazza tutti i valori tra 0 e 1, portandoli a 1"
      ],
      "metadata": {
        "id": "Q-T6qLA8nn-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complement Naive Bayes\n",
        "\n",
        "Variante del multinomial, utile per dataset sbilanciati, quindi con una delle calssi del target molto più presente dell'altra"
      ],
      "metadata": {
        "id": "yLaxBV8SpINv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 0\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-modelli-e-algoritmi/main/datasets/spam_unbalanced.csv\"\n",
        "\n",
        "df = pd.read_csv(BASE_URL)\n",
        "df.head()\n",
        "df[\"SPAM\"].value_counts() #molto sbilanciato, maggioranza di messaggi non sono di spam"
      ],
      "metadata": {
        "id": "HwKOtDxCpKaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#usiamo funzione bow di sklearn...\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "-gZ4Hf8Hpiiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = df[\"MESSAGE\"]\n",
        "y = df[\"SPAM\"]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n",
        "\n",
        "bow = CountVectorizer(stop_words=\"english\", max_features=1000) #usiamo le 1000 più comuni nel dizionario\n",
        "\n",
        "x_train = bow.fit_transform(x_train)\n",
        "x_test = bow.transform(x_test)\n",
        "x_train.shape # 70%,30%"
      ],
      "metadata": {
        "id": "XnRTtswYp7fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "comNB = ComplementNB()\n",
        "comNB.fit(x_train, y_train)\n",
        "print(classification_report(y_test, comNB.predict(x_test)))\n",
        "#precision bassa, recall alta, quindi il modello sta classificando troppe cose come appartenenti alla classe positiva\n",
        "#vediamo confusion matrix per vedere se è così...\n",
        "confusion_matrix(y_test, comNB.predict(x_test)) # molti falsi positivi, 18, mentre i falsi negativi sono solo 3\n",
        "\n",
        "#vediamo come sarebbe andata con il multinomial, che non è fatto per modelli sbilanciati\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train, y_train)\n",
        "print(classification_report(y_test, comNB.predict(x_test)))\n",
        "#sembra comportarsi meglio, anche se il dataset è sbilanciato, non c'è sempre una regola empirica"
      ],
      "metadata": {
        "id": "uXDB_R9Bq0tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Naive Bayes\n",
        "\n",
        "Variante per variabili di tipo categorico"
      ],
      "metadata": {
        "id": "v_qPKHzar5nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 0\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/\"\n",
        "\n",
        "df = pd.read_csv(BASE_URL + \"housing.csv\", usecols=[\"ZN\", \"CHAS\", \"RAD\", \"PRICE\"])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "woRHfZOpr8WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "x= df.drop(\"PRICE\", axis=1).values\n",
        "y = df[\"PRICE\"].values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n",
        "\n",
        "#NOTA\n",
        "#OrdinalEncoder potrebbe dare errore se nel test compaiono categorie non presenti in quello di train, qundi usiamo apposito parametro\n",
        "ordenc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
        "x_train = ordenc.fit_transform(x_train)\n",
        "x_test = ordenc.transform(x_test)\n",
        "\n",
        "#creazione del modello\n",
        "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train)\n",
        "gnb.score(x_test, y_test)\n",
        "#dà errore, in quanto naive bayes non è indicato per regressioni ma classificazioni\n",
        "#riformuliamo il dataset per avere delle categorie fisse...\n",
        "\n",
        "#usiamo quantili per definire features...\n",
        "df[\"PRICE\"].quantile(0)\n",
        "df[\"PRICE\"].quantile(1)\n",
        "#...\n",
        "\n",
        "def price_to_category(price):\n",
        "  CATEGORIES = [\"VERY CHEAP\", \"CHEAP\", \"AVERAGE\", \"EXPENSIVE\", \"VERY EXPENSIVE\"]\n",
        "\n",
        "  for i in range(1,5):\n",
        "    if price < df[\"PRICE\"].quantile(0.2*i): #diviso in 5, quindi andiamo a step di 0.2\n",
        "      return CATEGORIES[i]\n",
        "\n",
        "  return CATEGORIES[-1]\n",
        "\n",
        "#così abbiamo trasformato problema di regressione in classificaizone\n",
        "\n",
        "df[\"PRICE\"] = df[\"PRICE\"].apply(price_to_category)\n",
        "df.head()\n",
        "\n",
        "x= df.drop(\"PRICE\", axis=1).values\n",
        "y = df[\"PRICE\"].values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n",
        "\n",
        "ordenc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan)\n",
        "x_train = ordenc.fit_transform(x_train)\n",
        "x_test = ordenc.transform(x_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train)\n",
        "gnb.score(x_test, y_test)\n",
        "#otteniamo 0.34, considerato 5 classi, random è attorno 20%, quindi un pelo meglio della casualità\n",
        "\n",
        "#proviamo invece il categorial...\n",
        "catnb = CategoricalNB()\n",
        "catnb.fit(x_train, y_train)\n",
        "catnb.score(x_test, y_test)\n",
        "#otteniamo valore migliore!"
      ],
      "metadata": {
        "id": "-zNa_QCLsz-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes (esercitazione)\n",
        "\n",
        "Spam filtering"
      ],
      "metadata": {
        "id": "AUL2UCo-YSr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris, make_gaussian_quantiles, make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, classification_report, confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "#preprocessing step\n",
        "df = pd.read_csv(\"spam.csv\", encoding=\"ISO-8859-1\") #usiamo encoding apposito perchè il dataset è molto sporco come caratteri\n",
        "df.head() #target \"v1\" = ham oppure spam, \"v2\" variabile target, il resto non importa\n",
        "\n",
        "df = df[[\"v2\",\"v1\"]]\n",
        "df = df.rename(columns={\"v2\": \"MESSAGE\", \"v1\":\"SPAM\"})\n",
        "df.head()\n",
        "\n",
        "#endoding della variabile target\n",
        "classes_encoding = {\"spam\":1, \"ham\": 0}\n",
        "df[\"SPAM\"] = df[\"SPAM\"].map(lambda x: classes_encoding[x])\n",
        "df.head()\n",
        "\n",
        "#hold out\n",
        "x = df[\"MESSAGE\"].values\n",
        "y = df[\"SPAM\"].values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n",
        "x_train.shape\n",
        "\n",
        "#iniziamo con modello Bernnoulli (feature descritte da distribuzione binaria)\n",
        "#usiamo fuzione HashingVectorizer che converte testo in matrice di token\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "enc = HashingVectorizer(n_features=1000, stop_words=\"english\")\n",
        "x_train = enc.fit_transform(x_train) #matrice sparsa composta per lo più da zeri\n",
        "x_test = enc.transform(x_test)\n",
        "\n",
        "x_train[0:].toarray() #per vederla come matrice classica\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(x_train, y_train)\n",
        "print(classification_report(y_test, bnb.predict(x_test))) #il modello non è male, vediamo con multinomiale...\n",
        "\n",
        "#proseguiamo analisi con modello di Bernoulli\n",
        "#dobbiamo trattare token per frequenza e non solo on modalità binaria\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_STATE)\n",
        "\n",
        "enc = CountVectorizer(max_features=1000, stop_words=\"english\")\n",
        "x_train = enc.fit_transform(x_train)\n",
        "x_test = enc.transform(x_test)\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train, y_train)\n",
        "print(classification_report(y_test, mnb.predict(x_test))) #il modello è anche migliore del precedente...\n",
        "\n",
        "#non abbiamo controllato se il dataset era bilanciato o no sul target...\n",
        "df[\"SPAM\"].value_counts() #molto sbilanciato...\n",
        "\n",
        "#...quindi vale la pena testare con il complement\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "cnb = ComplementNB()\n",
        "cnb.fit(x_train, y_train)\n",
        "print(classification_report(y_test, mnb.predict(x_test))) #in questo caso era meglio il multinomial, troppo penalizzata la classe predominante\n",
        "\n",
        "#proviamo con 10000 feature invece di 1000...c'è un ulteriore miglioramento\n",
        "\n",
        "#test pratico del modello...\n",
        "\n",
        "def is_spam(text):\n",
        "  text_enc = enc.transform([text])\n",
        "  pred = mnb.predict(text_enc)[0] #primo e unico valore\n",
        "  return pred==1\n",
        "\n",
        "is_spam(\"Sex sex sex\") #true\n",
        "is_spam(\"nature air water\") #false\n"
      ],
      "metadata": {
        "id": "xQXEv73ZYVn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Support Vector Machines (teoria)"
      ],
      "metadata": {
        "id": "9UdM8VCmd6v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Maximal margin classifier"
      ],
      "metadata": {
        "id": "6RtTLEmaelWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "RANDOM_SEED = 6\n",
        "\n",
        "x,y = make_blobs(n_samples=40, centers=2, random_state=RANDOM_SEED) #dataset di prova con 2 classi\n",
        "plt.scatter(x[:,0], x[:,1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "\n",
        "#usiamo un SVM lineare (non esiste funzione Maximal margin classifier in sklearn)\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel=\"linear\")\n",
        "model.fit(x, y)\n",
        "print(model.support_vectors_)\n",
        "\n",
        "#per unire i due grafici....\n",
        "ax = plt.gca()\n",
        "\n",
        "#rapprestinamo i soft margin, così vediamo i vettori di supporto\n",
        "#senza (plot_method=\"contour\") i colori mostrano le probabilità di appartenenza alle due classi in base alle sfumature\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "DecisionBoundaryDisplay.from_estimator(\n",
        "    model,\n",
        "    x,\n",
        "    ax=ax,\n",
        "    alpha=0.5,\n",
        "    plot_method=\"contour\",\n",
        "    levels=[-1,0,1],\n",
        "    linestyles=[\"--\",\"-\",\"--\"],\n",
        "    colors=\"k\"\n",
        ")\n",
        "\n",
        "ax.scatter(\n",
        "    model.support_vectors_[:,0],\n",
        "    model.support_vectors_[:,1],\n",
        "    s=100,\n",
        "    linewidth=1,\n",
        "    facecolors = \"none\",\n",
        "    edgecolors = \"k\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ImUQutmDeoLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Linear SVM"
      ],
      "metadata": {
        "id": "4smUKiKOiFpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "NVVbNYM7iH38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(model, data, sv=None):\n",
        "\n",
        "    X, Y = data\n",
        "    h = .02\n",
        "\n",
        "    x_min, x_max = X[:, 0].min()-.1, X[:, 0].max()+.1\n",
        "    y_min, y_max = X[:, 1].min()-.1, X[:, 1].max()+.1\n",
        "\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    X_m = X[Y==1]\n",
        "    X_b = X[Y==0]\n",
        "    plt.scatter(X_b[:, 0], X_b[:, 1], c=\"green\",  edgecolor='white')\n",
        "    plt.scatter(X_m[:, 0], X_m[:, 1], c=\"red\",  edgecolor='white')\n",
        "\n",
        "    if sv is not None:\n",
        "          plt.scatter(sv[:, 0], sv[:, 1], c=\"blue\",  edgecolor='white')\n",
        "\n",
        "def classifier_report(model, data):\n",
        "  X, y = data\n",
        "  y_pred = model.predict(X)\n",
        "  report = classification_report(y_pred, y)\n",
        "  print(report)"
      ],
      "metadata": {
        "id": "xFHpVrOwiX-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generiamo un dataset...\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, random_state=RANDOM_SEED)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "oLxL6dW4ifz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creiamo SVM lineare (che non ha proprietà del kernel)\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC()\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test)) #il modello è molto buono\n",
        "\n",
        "plot_decision_boundary(svc, (X_train, y_train))\n",
        "\n",
        "#vediamo ora i support vector usando SVC\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(kernel=\"linear\")\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "\n",
        "plot_decision_boundary(svc, (X_train, y_train), sv=svc.support_vectors_)\n",
        "#molti elementi, cioè tutti quella nella sezione sbagliata...\n",
        "\n",
        "#vediamo cosa succede toglierndo outlier \"a occhio\"...\n",
        "X_train[:,0]>3.5\n",
        "outlier_index = np.where(X_train[:,0]>3)\n",
        "X_train[outlier_index,:]\n",
        "X_train2 = np.delete(X_train, outlier_index, axis=0)\n",
        "y_train2 = np.delete(y_train, outlier_index)\n",
        "\n",
        "X_train2.shape #68 elementi\n",
        "X_train.shape #70 elementi\n",
        "\n",
        "#come  metriche è uguale..\n",
        "svc = SVC(kernel=\"linear\")\n",
        "svc.fit(X_train2, y_train2)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "\n",
        "plot_decision_boundary(svc, (X_train2, y_train2), sv=svc.support_vectors_)\n",
        "\n",
        "#ora proviamo caso estremo, rimuovendo tutti gli outliers utilizzando lo z-score > deviazione standard\n",
        "from scipy import stats\n",
        "\n",
        "outliers_map = np.abs(stats.zscore(X_train)) < X_train.std(axis=0)\n",
        "outliers_map = outliers_map[:,0] & outliers_map[:,1]\n",
        "\n",
        "X_train_filtered = X_train[outliers_map]\n",
        "y_train_filtered = y_train[outliers_map]\n",
        "\n",
        "X_train_filtered.shape #48 elementi rimasti\n",
        "\n",
        "svc = SVC(kernel=\"linear\")\n",
        "svc.fit(X_train_filtered, y_train_filtered)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "\n",
        "plot_decision_boundary(svc, (X_train_filtered, y_train_filtered), sv=svc.support_vectors_)"
      ],
      "metadata": {
        "id": "AXwZB7ujjOlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Esempio SVM con vari Kernel"
      ],
      "metadata": {
        "id": "cnE5hY7Pmjz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "_aPDMwk9mmzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(model, train_set, test_set, sv=None):\n",
        "\n",
        "    #plt.figure(figsize=figsize)\n",
        "\n",
        "    if(model):\n",
        "        X_train, Y_train = train_set\n",
        "        X_test, Y_test = test_set\n",
        "        X = np.vstack([X_train, X_test])\n",
        "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n",
        "                             np.arange(y_min, y_max, .02))\n",
        "\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "        else:\n",
        "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        plt.contourf(xx, yy, Z, alpha=.8)\n",
        "\n",
        "    plt.scatter(X_train[:,0], X_train[:,1], c=Y_train)\n",
        "    plt.scatter(X_test[:,0], X_test[:,1], c=Y_test, alpha=0.6)\n",
        "\n",
        "    if sv is not None:\n",
        "      plt.scatter(sv[:, 0], sv[:, 1], c=\"blue\",  edgecolor='white')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def classifier_report(model, data):\n",
        "  X, y = data\n",
        "  y_pred = model.predict(X)\n",
        "  report = classification_report(y_pred, y)\n",
        "  print(report)"
      ],
      "metadata": {
        "id": "lPJ-H2qqmtvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, y = make_circles(noise=0.2, factor=0.5, random_state=RANDOM_SEED) #generazione di un dataset con parte circolare all'interno\n",
        "plt.scatter(X[:,0],X[:,1],c=y)"
      ],
      "metadata": {
        "id": "Kp_mGgMnmuP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "nYDoT2axm3bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kerner lineare\n",
        "svc = SVC(kernel=\"linear\", probability=True) #possibile richiedere anche la probabilità\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test)) #pessimo modello...\n",
        "\n",
        "plot_decision_boundary(svc, (X_train, y_train), (X_test, y_test), sv=svc.support_vectors_)"
      ],
      "metadata": {
        "id": "04BbIJJQm302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kerner polinomiale\n",
        "svc = SVC(kernel=\"poly\", probability=True)\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "\n",
        "plot_decision_boundary(svc, (X_train, y_train), (X_test, y_test), sv=svc.support_vectors_) #un pò meglio del precedente"
      ],
      "metadata": {
        "id": "Rm399BjfpBCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kernel sigmoidale\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(kernel=\"sigmoid\", probability=True)\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "\n",
        "plot_decision_boundary(svc, (X_train, y_train), (X_test, y_test))\n",
        "#si comporta male, adatto a casi in cui la funzione è sigmoidale, quindi oscillante"
      ],
      "metadata": {
        "id": "iTCA9LDIpkHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kernel gaussiano\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(kernel=\"rbf\", probability=True)\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "\n",
        "plot_decision_boundary(svc, (X_train, y_train), (X_test, y_test)) #la migliore, con i valori che tendono a raggrupparsi"
      ],
      "metadata": {
        "id": "CRcGXioKqB7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YDxXeOVlo67X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Support Vector Machines (esercitazione)\n",
        "\n",
        "Previsione infarto cardiaco"
      ],
      "metadata": {
        "id": "jNvcaRZIo8tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "RANDOM_STATE = 0\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-modelli-e-algoritmi/main/datasets/heart.csv\"\n",
        "\n",
        "df = pd.read_csv(BASE_URL)\n",
        "df.head()\n",
        "\n",
        "#valutazione dei dati:\n",
        "#one hot encoding per varibili categoriche non ordinate\n",
        " #scale diverse sarà  da standardizzare\n",
        "df.describe()\n",
        "\n",
        "X = df.drop([\"output\"], axis=1).values\n",
        "y = df[\"output\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=RANDOM_STATE)\n",
        "X_train.shape()\n",
        "X_test.shape()\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "    [\n",
        "        (\"ohe\", OneHotEncoder(), [2]),\n",
        "    ],\n",
        "    remainder=\"passthrough\" # per non eliminare le altre colonne\n",
        ")\n",
        "\n",
        "X_train = ct.fit_transform(X_train) #quindi abbiamo ora le colonne in più\n",
        "X_test = ct.transform(X_test)\n",
        "\n",
        "std = StandardScaler()\n",
        "X_train = std.fit_transform(X_train)\n",
        "X_test = std.transform(X_test)\n",
        "print (X_train.mean(), X_train.std()) #media intorno allo zero e std a 1"
      ],
      "metadata": {
        "id": "sBjY6hiUMBGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_report(model, data):\n",
        "  X, y = data\n",
        "  y_pred = model.predict(X)\n",
        "  report = classification_report(y, y_pred)\n",
        "  print(report)\n",
        "  print(confusion_matrix(y, y_pred))"
      ],
      "metadata": {
        "id": "sm1tTQhEMnZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modello baseline - Regressione logistica\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "classifier_report(lr, (X_test, y_test))\n",
        "#non è male, ma dobbiamo attenzionare mancati avvisi di infarti\n",
        "#quindi precision ci dice quanti positivi erano positivi, invece recall ci dice i casi veramente riconosciuti quindi FalseNegative, in questo caso vale 4...\n",
        "\n",
        "#Support Vector Classifier lineare (probabimente avrà comportamento simile al precedente)\n",
        "svc = SVC(kernel=\"linear\")\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "#uguale al preceente...proseguiamo con altri tentativi...\n",
        "\n",
        "#kernel SVM\n",
        "svc = SVC(kernel=\"rbf\") #gaussiano\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "#ancora due casi....proviamo a applicare pesi per penalizzare...\n",
        "svc = SVC(kernel=\"rbf\", class_weight={1:1.5, 0:0.5})\n",
        "svc.fit(X_train, y_train)\n",
        "classifier_report(svc, (X_test, y_test))\n",
        "#non basta e precisione peggiorata...probabilmente ci sono outliers tra le due classi\n",
        "\n",
        "#si potrebbe usare un valore di soglia più basso, invece di 0.5 che è il default"
      ],
      "metadata": {
        "id": "dqh9FAvTNggX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Networks (teoria)"
      ],
      "metadata": {
        "id": "Z7w1TJvfMq6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "!wget https://raw.githubusercontent.com/ProfAI/machine-learning-modelli-e-algoritmi/main/script/viz.py\n",
        "\n",
        "from viz import plot_decision_boundary\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "#generazione dei dati con relazione molto complessa su due classi\n",
        "#non c'è nessuna retta di separazione tra le due classi\n",
        "X, y = make_moons(n_samples=100, shuffle=True, noise=0.25, random_state=RANDOM_SEED)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "\n",
        "#preprocessing dei dati\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=RANDOM_SEED)\n",
        "\n",
        "#normalizzazione\n",
        "mms = MinMaxScaler()\n",
        "x_train = mms.fit_transform(x_train)\n",
        "x_test = mms.transform(x_test)\n",
        "\n",
        "#esperimento con modello lineare, il perceptor\n",
        "from sklearn.linear_model import Perceptron\n",
        "model = Perceptron() #unica differenza da regressione logistica è che quella usa sigmoide come funzione attivazione finale, qui si usa step function\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test, y_test) #accuracy, 0.85, retta che separa parzialmente le due classi...\n",
        "plot_decision_boundary(model, (x_train, y_train), (x_test, y_test)) #retta senza soft margin, il percettrone è 0 o 1 data la step function\n",
        "\n",
        "#esperimento con regressione logistica lineare, che però tiene conto probabilità output\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.score(X_test, y_test) #sempre 0.85\n",
        "plot_decision_boundary(lr, (x_train, y_train), (x_test, y_test))\n",
        "\n",
        "#proviamo multi-layer perceptor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,), random_state=RANDOM_SEED) #3 strati con 100 nodi ciascuno\n",
        "mlp.fit(X_train, y_train)\n",
        "mlp.score(X_test, y_test) #0.9\n",
        "plot_decision_boundary(mlp, (x_train, y_train), (x_test, y_test)) #il modello commette piccoli errori, ma approssima meglio la distribuzione\n",
        "\n",
        "#sezioniamo la rete per vedere i pesi...\n",
        "len(mlp.coefs_) #numero strati nascosti + input\n",
        "mlp.coefs_[0] #pesi tra strato input e primo nascosto\n",
        "len(mlp.intercepts_) #bias di ogni strato\n",
        "\n",
        "weights = 0\n",
        "for i in range(len(mlp.coefs_)):\n",
        "  coefs = mlp.coefs_[i].size\n",
        "  bias = mlp.intercepts_[i].size\n",
        "\n",
        "  tot = coefs+bias\n",
        "  print(f\"Layer {i} => Pesi={coefs} Bias={bias} Totale={tot}\")\n",
        "\n",
        "  weights += tot\n",
        "\n",
        "print(f\"Pesi totali della rete={weights}\")\n",
        "#al primo step 200 pesi, 2 feature di ingresso, ognuna a 100 neuroni\n",
        "#al secondo step 100 neuroni che si collegano ad altri 100, quindi 10000, bias rimane sempre uno per quello successivo\n",
        "#l'ultimo strato nascosto con l'output avrà 100 pesi perchè è uno l'output\n",
        "#le reti neurali non sono interpretabili e richiedono molti dati!\n",
        "\n",
        "#a cosa serve la funzione di attivazione?\n",
        "#proviamo a toglierla, tornando x=x\n",
        "hiddel_layer_size = (100,)*10 #10 strati nascosti con 100 nodi\n",
        "mlp = MLPClassifier(hidden_layer_sizes=hiddel_layer_size, activation=\"identity\", random_state=RANDOM_SEED)\n",
        "mlp.fit(X_train, y_train)\n",
        "mlp.score(X_test, y_test) #0.85\n",
        "plot_decision_boundary(mlp, (x_train, y_train), (x_test, y_test))\n",
        "#togliendo funzione di attivazione, abbiamo tolto possibilità di avere funzioni non lineari, quindi modello lineare ricade in quello visto sopra"
      ],
      "metadata": {
        "id": "KJARNb6QkL1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Networks (esercitazione)\n",
        "\n",
        "Classificazione di cifre scritte a mano"
      ],
      "metadata": {
        "id": "8_z05j0fTva8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#l'ultimo elemento è la classe di appartenenza, i precedenti sono il valore numerico dell'immagine (8x8 pixel)\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes"
      ],
      "metadata": {
        "id": "yjVeLAsBugih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "### Caricamento dei dati...\n",
        "from numpy import genfromtxt\n",
        "\n",
        "arr = genfromtxt('optdigits.tra', delimiter=',')\n",
        "arr.shape #3823 righe e 65 colonne (da 0 a 63 i valori dei pixel, l'ultimo è il target)\n",
        "\n",
        "#stampiamo un immagine di esempio\n",
        "sample_num = 100\n",
        "print(arr[sample_num,-1])\n",
        "plt.imshow(arr[sample_num,:-1].reshape(8,8))\n",
        "\n",
        "### Processiamo i dati...\n",
        "x = arr[:,:-1] #3823,64\n",
        "y = arr[:,-1] #3823,\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=RANDOM_SEED)\n",
        "x_train.max() #max è 16, quindi non dividiamo per 255 ma 16 in questo caso\n",
        "x_max = x_train.max()\n",
        "x_train/=x_max\n",
        "x_test/=x_max\n",
        "x_train.max() #max è normalizzato a 1\n",
        "\n",
        "### Multilayer Perceptron...\n",
        "mlp = MLPClassifier(random_state=RANDOM_SEED)\n",
        "mlp.fit(x_train, y_train)\n",
        "mlp.score(x_test, y_test) #0.97, accuracy molto buona\n",
        "y_pred = mlp.predict(x_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "#avendo più classi abbiamo spaccato per ognuna\n",
        "\n",
        "#ricaviamo le immagini che non sono state interpretate correttamente...\n",
        "errors_mask = y_pred!=y_test\n",
        "x_errors = x_test[errors_mask]\n",
        "y_errors = y_test[errors_mask]\n",
        "y_errors_pred = y_pred[errors_mask]\n",
        "x_errors.shape #18 errori\n",
        "\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "fig.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "for i in range(x_errors.shape[0]):\n",
        "  plot = fig.add_subplot(4, 5, i+1)\n",
        "  plt.imshow(x_errors[i,:].reshape(8,8))\n",
        "  plot.text(0, 7, f\"Class: {int(y_errors[i])}\", fontsize=14, fontdict={'weight': 'bold'})\n",
        "  plot.text(0, 6, f\"Predicted: {int(y_errors_pred[i])}\", fontsize=14, fontdict={'weight': 'bold'})\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "###Proviamo cosa sarebbe successo con regressione logistica semplice\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "lr.score(x_test, y_test)\n",
        "#accuracy del 0.97"
      ],
      "metadata": {
        "id": "jok6pcGCUxcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbors (teoria)"
      ],
      "metadata": {
        "id": "xW2OSPAdNTYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "UnqFgl2eNXKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classificazione binaria di test\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, random_state=RANDOM_SEED)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "metadata": {
        "id": "cPWEeEF8Nhjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=RANDOM_SEED)"
      ],
      "metadata": {
        "id": "hN14H-jtNqpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "Ks = [1,2,3,4,5,7,10,12,15,20,30,40,50] # proviamo vari valori di K\n",
        "\n",
        "for K in Ks:\n",
        "\n",
        "    print(\"K=\"+str(K))\n",
        "    knn = KNeighborsClassifier(n_neighbors=K)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_train = knn.predict(X_train)\n",
        "    y_prob_train = knn.predict_proba(X_train)\n",
        "\n",
        "    y_pred = knn.predict(X_test)\n",
        "    y_prob = knn.predict_proba(X_test)\n",
        "\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    loss_train = log_loss(y_train, y_prob_train)\n",
        "    loss_test = log_loss(y_test, y_prob)\n",
        "\n",
        "    print(\"ACCURACY: TRAIN=%.4f TEST=%.4f\" % (accuracy_train,accuracy_test))\n",
        "    print(\"LOG LOSS: TRAIN=%.4f TEST=%.4f\" % (loss_train,loss_test))\n",
        "\n",
        "#10 pare il migliore valore, testiamolo....\n",
        "knn = KNeighborsClassifier(n_neighbors=10)\n",
        "knn.fit(X_train, y_train)\n",
        "x = [0,0] #punto centrale grafico\n",
        "y_pred = knn.predict([x])\n",
        "y_pred #predetto classe gialla, giusto\n",
        "\n",
        "#viualizziamo i valori vicini...\n",
        "distances, neighbors = knn.kneighbors([x])\n",
        "neighbors #indici nel dataset dei valori più vicini\n",
        "\n",
        "X_neighbors = X_train[neighbors][0]\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.scatter(x[0], y[0], c=\"green\")\n",
        "plt.scatter(X_neighbors[:,0], X_neighbors[:,1], facecolors='none', edgecolors='r')\n",
        "\n",
        "#solo uno viola, infatti:\n",
        "knn.predict_proba([x]) #array([[0.1, 0.9]])"
      ],
      "metadata": {
        "id": "Wt0xNN8zNrV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Radius Nearest Neighbors\n",
        "\n",
        "Siccome basato su distanze fonadmentale normalizzare"
      ],
      "metadata": {
        "id": "JxXJd9rhhCPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mms = MinMaxScaler()\n",
        "X_train_norm = mms.fit_transform(X_train)\n",
        "X_test_norm = mms.transform(X_test)\n",
        "\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "\n",
        "Rs = [.2, .3, .4, .5, .6, .7, .8, .9, 1]\n",
        "\n",
        "for R in Rs:\n",
        "\n",
        "    print(\"Radius=\"+str(R))\n",
        "    rnn = RadiusNeighborsClassifier(radius=R)\n",
        "    rnn.fit(X_train_norm, y_train)\n",
        "\n",
        "    y_pred_train = rnn.predict(X_train_norm)\n",
        "    y_prob_train = rnn.predict_proba(X_train_norm)\n",
        "\n",
        "    y_pred = rnn.predict(X_test_norm)\n",
        "    y_prob = rnn.predict_proba(X_test_norm)\n",
        "\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    loss_train = log_loss(y_train, y_prob_train)\n",
        "    loss_test = log_loss(y_test, y_prob)\n",
        "\n",
        "    print(\"ACCURACY: TRAIN=%.4f TEST=%.4f\" % (accuracy_train,accuracy_test))\n",
        "    print(\"LOG LOSS: TRAIN=%.4f TEST=%.4f\" % (loss_train,loss_test))\n",
        "\n",
        "#il migliore è con raggio .2\n",
        "r = .2\n",
        "rnn = RadiusNeighborsClassifier(radius=r)\n",
        "rnn.fit(X_train_norm, y_train)\n",
        "\n",
        "#mostriamo il raggio..\n",
        "x = [0,0]\n",
        "x = mms.transform([x])\n",
        "y_pred = rnn.predict(x)\n",
        "y_pred\n",
        "\n",
        "fig, ax = plt.subplots() # note we must use plt.subplots, not plt.subplot\n",
        "\n",
        "ax.scatter(X_train_norm[:,0], X_train_norm[:,1], c=y_train)\n",
        "ax.scatter(x[0,0], x[0,1], c=\"green\")\n",
        "\n",
        "circle = plt.Circle((x[0,0], x[0,1]), r, color='r', fill=None)\n",
        "\n",
        "ax.add_patch(circle)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GME9ERRWhGDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##K-Nearest Neighbors (esercitazione)\n",
        "\n",
        "Face recognition (richiesta accuracy del 80%)"
      ],
      "metadata": {
        "id": "UWTTCB3th-px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scarichiamo il dataset con immagini (rappresentati come numeri)\n",
        "#c'è una cartella per persona, con 10 immagini della persona da varie angolazioni\n",
        "!wget https://github.com/ProfAI/machine-learning-modelli-e-algoritmi/raw/main/datasets/olivetti_faces.zip\n",
        "!unzip olivetti_faces.zip"
      ],
      "metadata": {
        "id": "HBLQc9kuiByg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from os import listdir\n",
        "import cv2\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "img_size = (64, 64)"
      ],
      "metadata": {
        "id": "JrupPrblkpox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"olivetti_faces/0/0.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "img.shape # (64, 64, 3) base, altezza canale gray\n",
        "plt.imshow(img, cmap=\"gray\")"
      ],
      "metadata": {
        "id": "1F-a8ZyflWaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER = \"olivetti_faces\"\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for dir in listdir(FOLDER):\n",
        "  path = FOLDER+\"/\"+dir+\"/\"\n",
        "  for f in listdir(path):\n",
        "    if \".jpg\" in f:\n",
        "      x = cv2.imread(path+f, cv2.IMREAD_GRAYSCALE)\n",
        "      X.append(x)\n",
        "      y.append(dir)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "id": "oEhVSHDFk74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[0], cmap=\"gray\")"
      ],
      "metadata": {
        "id": "tk_nPpt4nHb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) #in modo da avere un unica dimenisone, così abbiamo 400 osservazione con 4096 pixel (64*64)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "VgO8QyJSnHxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=RANDOM_SEED)\n",
        "X_train = X_train/255 #per le immagini basta normalizzare facendo divisione per 255 in modo da avere valori tra 0 e 1\n",
        "X_test = X_test/255\n",
        "\n",
        "#potrebbe succedere che una classe non venga messa nel train ma solo nel test, anche se di solito train_test_split si occupa di questo\n",
        "(1-np.isin(y_test, y_train)).sum()"
      ],
      "metadata": {
        "id": "LoaNir4tnnOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rilanciamo valutazione della teoria e scegliamo k=3 come migliore tra accuracy (evitando overfitting)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "knn.score(X_test, y_test)\n",
        "\n",
        "#per testare con una foto, necessario ritagliare solo il volto\n",
        "!wget https://www.antoniocapraro.it/public/Files/rif000002/135/viso-perfetto-4.jpg #da ridimensionare\n",
        "x = cv2.imread(\"viso-perfetto-4.jpg\", cv2.IMREAD_GRAYSCALE)\n",
        "plt.imshow(x, cmap=\"gray\")\n",
        "\n",
        "x = x.reshape(img_size[0]*img_size[1])\n",
        "x = x/255\n",
        "\n",
        "x.shape\n",
        "\n",
        "y_pred = knn.predict([x])\n",
        "y_pred\n",
        "\n",
        "y_proba = knn.predict_proba([x])[0]\n",
        "\n",
        "#non emerge nessun sosia\n",
        "for i in range(y_proba.shape[0]):\n",
        "  print(f\"Person {knn.classes_[i]} = {y_proba[i]} probability\")"
      ],
      "metadata": {
        "id": "sxN8JNLeoama"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}