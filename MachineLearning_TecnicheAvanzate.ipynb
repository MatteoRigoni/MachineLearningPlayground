{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHLgVgL1hK/KoyF/XaIG+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoRigoni/MachineLearningPlayground/blob/master/MachineLearning_TecnicheAvanzate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning degli iperparamteri"
      ],
      "metadata": {
        "id": "DPTMPE5jU9s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "import pandas as pd\n",
        "\n",
        "import scipy\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # per printare su una stessa cella"
      ],
      "metadata": {
        "id": "lp6N_uEEWgP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\n",
        "    'https://archive.ics.uci.edu/ml/'\n",
        "    'machine-learning-databases'\n",
        "    '/breast-cancer-wisconsin/wdbc.data',\n",
        "    header=None\n",
        ")\n",
        "df.head()\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "bq6WBrmGWtYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X, y = df.iloc[:, 2:].values, df.iloc[:, 1].values\n",
        "len(X), len(y)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "le.classes_ #binario\n",
        "len(y)"
      ],
      "metadata": {
        "id": "rireZmE4Yj3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split del dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=.2, stratify=y, random_state=1\n",
        ")\n",
        "len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "xvr3-trNY-aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creazione pipeline di linear regression\n",
        "pipe_lr = make_pipeline(\n",
        "    StandardScaler(), #scala prima tutti gli input\n",
        "    PCA(n_components=2),\n",
        "    LogisticRegression()\n",
        ")\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "y_pred = pipe_lr.predict(X_test)\n",
        "test_acc = pipe_lr.score(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.3f}')"
      ],
      "metadata": {
        "id": "QGTjYYGOZRgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cross Validation\n",
        "\n",
        "Nel processo di tuning degli iperparametri del modello, è cruciale non usare sempre lo stesso dataset di test, perchè questo porterebbe a trovare una soluzione ottima con alto bias. La corretta procedura, invece, deve essere quella di usare un dataset di validation estratto dal dataset di training. Una possibilità sarebbe quella di usare la tecnica del holdout, in cui il dataset totale viene suddiviso in 3 sotto insiemi: [train, validation, test], in genere con percentuali del tipo [70%, 10%, 20%], rispettivamente. Tuttavia, questa tecnica dipende fortemente dal unico modo utilizzato per splittare il dataset.\n",
        "\n",
        "Una pratica ancora più performante è la tecnica dello Stratified Cross-Validation. In tale tecnica, il dataset iniziale viene suddivo in train e test, successivamente (in fase di evaluation) il dataset di train viene suddiviso in K folds, per cui K-1 vengono usati come dataset di training e il K-esimo come validation. La procedura viene ripetuta un numero K di volte, in cui il K-esimo dataset di validation è sempre differente dal caso precedente. Lo score finale sarà ottenuto prendendo la media delle K iterazioni. Inoltre, il metodo assicura che, in ogni iterazione il numero delle classi rimane constante (stratified), in modo da trainare i K modelli, in situazioni simili, per quanto riguarda il numero di labels.\n",
        "\n",
        "A fine della procedura, viene trainato un unico modello, usando le configurazioni degli iperparametri ottimi, ed utilizzando l'intero dataset di training (train + validation == 80% dei dati) e usando il dataset di test (mai utilizzato finora) per valutare l'errore di generalizzazione.\n",
        "\n"
      ],
      "metadata": {
        "id": "Oq_yJwoYlCCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "KFold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
        "\n",
        "scores = []\n",
        "for k, (train, test) in enumerate(KFold):\n",
        "    _ = pipe_lr.fit(X_train[train], y_train[train])\n",
        "    scores.append(pipe_lr.score(X_train[test], y_train[test]))\n",
        "    print(\n",
        "        f'Fold {k+1:02} '\n",
        "        f'Class distr. : {np.bincount(y_train[train])} '\n",
        "        f'CV score: {scores[-1]:.3f}')\n",
        "\n",
        "\n",
        "print(f'CV accuracy averaged: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')\n",
        "# the same thing, but less verbose ...\n",
        "scores = cross_val_score(\n",
        "    estimator=pipe_lr,\n",
        "    X=X_train,\n",
        "    y=y_train,\n",
        "    cv=10, # 10-folds\n",
        "    n_jobs=-1 # exploits all available cpus\n",
        ")\n",
        "print(f'CV accuracy scores:\\n{scores}\\n')\n",
        "print(f'CV accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}')"
      ],
      "metadata": {
        "id": "S7OgadiTlHxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validation curve\n",
        "\n",
        "La validation curve visualizza l'accuratezza, sul dataset di training e di validation, al variare dei valori di alcuni iperparametri del modello. Il suo plot aiuta ad individuare, in modo grafico, il range di valori entro i quali, gli iperparametri danno un accuratezza sul validation test ottima.\n"
      ],
      "metadata": {
        "id": "b_KW3oQ8krKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_validation_curve(estimator, X, y, param_range, param_name, cv=10):\n",
        "    \"\"\"Validation curve plot: shows accuracy values for different values of a select parameters.\n",
        "\n",
        "    --Params\n",
        "     - estimator: a scikit-learn estimator,\n",
        "     - X, y: numpy ndarrays\n",
        "     - param_range: List, the range values the paramete must explor\n",
        "     - param_name: str, the name of the parameter\n",
        "     -cv: int, the number of folds for the stratified cross validation\n",
        "     \"\"\"\n",
        "\n",
        "    train_score, test_score = validation_curve(\n",
        "        estimator=estimator,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        param_range=param_range,\n",
        "        param_name=param_name,\n",
        "        cv=cv\n",
        "    )\n",
        "\n",
        "    train_avg, train_std = np.mean(train_score, axis=1), np.std(train_score, axis=1)\n",
        "    test_avg, test_std = np.mean(test_score, axis=1), np.std(test_score, axis=1)\n",
        "\n",
        "    plt.plot(param_range, train_avg, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
        "    plt.fill_between(param_range, train_avg + train_std, train_avg - train_std, alpha=.15, color='blue')\n",
        "    plt.plot(param_range, test_avg, color='green', marker='s', markersize=5, linestyle='--', label='Validation Accuracy')\n",
        "    plt.fill_between(param_range, test_avg + test_std, test_avg - test_std, alpha=.15, color='green')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.xlabel(f\"Parameter {param_name.split('__')[-1]}\")\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([0.8, 1.0])\n",
        "    plt.xscale('log')\n",
        "    plt.show()\n",
        "\n",
        "pipe_lr = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(penalty='l2', max_iter=10000)\n",
        ")\n",
        "plot_validation_curve(pipe_lr, X_train, y_train, [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], 'logisticregression__C')"
      ],
      "metadata": {
        "id": "ce2baprRlAYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grid search, per scelta degli iperparametri che sono impostati prima dell'addestramento\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipe_svc = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(random_state=1)\n",
        ")\n",
        "\n",
        "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "param_grid = [\n",
        "    {'svc__C': param_range,\n",
        "    'svc__kernel': ['linear']},\n",
        "\n",
        "    {'svc__C': param_range,\n",
        "    'svc__gamma': param_range,\n",
        "    'svc__kernel': ['rbf']}]\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe_svc,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=10,\n",
        "    refit=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gs = grid.fit(X_train, y_train)\n",
        "gs.best_score_\n",
        "gs.best_params_"
      ],
      "metadata": {
        "id": "fb4hgAtEbs1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = gs.best_estimator_\n",
        "_ = clf.fit(X_train, y_train)\n",
        "print(f'Test accuracy: {clf.score(X_test, y_test):.3f}')"
      ],
      "metadata": {
        "id": "VK_JfMzFdgFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random search ,utile per molti parametri, grid trova combinazione migliore ma molto oneroso elegato a range fissi\n",
        "param_range = scipy.stats.loguniform(0.0001, 1000.0) #prendiamo range con probabilità su distribuzione uniforme, quindi senza un bias prefissato\n",
        "param_range.rvs(10)"
      ],
      "metadata": {
        "id": "hWuJLOCiZTWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid = [\n",
        "    {\"svc__C\": param_range,\n",
        "    \"svc__kernel\": [\"linear\"]},\n",
        "    {\"svc__C\": param_range,\n",
        "    \"svc__gamma\": param_range,\n",
        "    \"svc__kernel\": [\"rbf\"]}\n",
        "]\n",
        "\n",
        "rs = RandomizedSearchCV(\n",
        "    estimator=pipe_svc,\n",
        "    param_distributions=param_grid, # note the *ditributions part\n",
        "    scoring='accuracy',\n",
        "    refit=True,\n",
        "    n_iter=20,\n",
        "    cv=10,\n",
        "    random_state=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "_ = rs.fit(X_train, y_train)\n",
        "rs.best_score_\n",
        "rs.best_params_\n",
        "\n",
        "#ha permesso di esplorare dei valori che con il grid search non avremmo esplorato"
      ],
      "metadata": {
        "id": "d9gaQsq_gYUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#estraiamo anche qui il  best estimator per fare il fit\n",
        "clf = rs.best_estimator_\n",
        "_ = clf.fit(X_train, y_train)\n",
        "print(f'Test accuracy: {clf.score(X_test, y_test):.3f}')"
      ],
      "metadata": {
        "id": "ZyWAJcWohmMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#infine proviamo halving random search,  che è come il random, ma non usa tutto il dataset in ogni iterazione, permettendo di velocizzare il processo\n",
        "# è una features sperimentale: bisogna importare questa macro, perchè l'API potrebbe cambiare senza deprecation cycle\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "# adesso importa il metodo\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "\n",
        "hs = HalvingRandomSearchCV(\n",
        "    estimator=pipe_svc,\n",
        "    param_distributions=param_grid,\n",
        "    n_candidates='exhaust', # scegli un numero di candidati tale da sfruttare tutto il dataset\n",
        "    resource='n_samples',\n",
        "    factor=1.5, # quanti candidati saranno scartati ad ogni iterazione (100%/1.5=66% rimarranno)\n",
        "    random_state=1,\n",
        "    n_jobs=-1\n",
        "    )\n",
        "\n",
        "_ = hs.fit(X_train, y_train)\n",
        "hs.best_score_\n",
        "hs.best_params_\n"
      ],
      "metadata": {
        "id": "_8DlLPp6h1-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = hs.best_estimator_\n",
        "_ = clf.fit(X_train, y_train)\n",
        "print(f'Test accuracy: {clf.score(X_test, y_test):.3f}')"
      ],
      "metadata": {
        "id": "V7iECbbRiBp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline"
      ],
      "metadata": {
        "id": "ErMpvoetXKZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data-preprocessing\n",
        "from sklearn.impute import SimpleImputer # imputation transformer for completing nan values\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "# feature selector\n",
        "from sklearn.decomposition import PCA\n",
        "# model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# composer and pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# load dataset\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "ad6Auc2DXL_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
        "X.dtypes"
      ],
      "metadata": {
        "id": "V9texy4hZ_4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = [col for col in X.columns if X[col].dtypes == 'float64']\n",
        "cat_features = [col for col in X.columns if X[col].dtypes == 'category']\n",
        "len(numerical_features), len(cat_features)"
      ],
      "metadata": {
        "id": "bkUY1fe4aKj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant')),\n",
        "        ('one-hot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, cat_features)\n",
        "        ]\n",
        ")\n",
        "\n",
        "my_pipe = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('pca', PCA()),\n",
        "        ('clf', LinearRegression())\n",
        "    ]\n",
        ")\n",
        "\n",
        "my_pipe"
      ],
      "metadata": {
        "id": "0qsdoRPVaate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=.2, random_state=1\n",
        ")"
      ],
      "metadata": {
        "id": "i8ft97m3cbvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_pipe.fit(X_train, y_train)\n",
        "my_pipe.predict(X_train)\n",
        "y_pred= my_pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "De725gsodCY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "#esportazione pipe\n",
        "joblib.dump(my_pipe, 'my_pipe.joblib')\n",
        "\n",
        "my_pipe = joblib.load('my_pipe.joblib')\n",
        "my_pipe.predict(X_train)"
      ],
      "metadata": {
        "id": "fdqYE_AVdh3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Serie storiche"
      ],
      "metadata": {
        "id": "Dnl-AtmhfBRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ProfAI/machine-learning-avanzato.git"
      ],
      "metadata": {
        "id": "9G0zvDZJfCNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "#packages\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tsa.api as tsa\n",
        "from statsmodels.tsa.arima_model import ARMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # per printare su una stessa cella\n"
      ],
      "metadata": {
        "id": "pNXuHHP1el3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"machine-learning-avanzato/datasets/coin_Bitcoin.csv\")\n",
        "df.head()\n",
        "len(df)\n",
        "df.Date.min(), df.Date.max()"
      ],
      "metadata": {
        "id": "NBch-GlVfi-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "df['Date'] = df.Date.apply(pd.to_datetime)\n",
        "df = df.loc[:,['Date','High','Low','Open','Close']]\n",
        "df= df.set_index('Date', drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wOEAYxVhf0E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Close.plot(title=\"BitCoin Closing Price\")\n",
        "#si evince che è non stazionaria, dimostriamolo\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "def test_ADF(x,  pvalue=0.05):\n",
        "  result = adfuller(x)\n",
        "  if result[1] <= pvalue:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "test_ADF(df.Close)\n",
        "\n",
        "#dobbiamo riportarlo a una serie stazionaria, visto che non  lo è, dato il p-value  alto, non possiamo escludere che non sia una serie non stazionaria\n",
        "#prendiamo i valori logaritmi...\n",
        "df['Close_log'] = np.log(df.Close)\n",
        "df['Close_log'].plot()\n",
        "test_ADF(df['Close_log'])\n",
        "#...non basta, non è ancora stazionaria\n",
        "df['Close_log_diff'] = df['Close_log'] - df['Close_log'].shift(1)\n",
        "df.dropna(inplace=True)\n",
        "_ = df['Close_log_diff'].plot()\n",
        "test_ADF(df['Close_log_diff'])\n",
        "#...ora risulta stazionaria, quindi con parametro 1\n",
        "\n",
        "#ora cerchiamo che valore deve avere per averlo autoregressivo\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "fix, ax = plt.subplots(1, sharex=True, figsize=(8,8))\n",
        "plot_acf(df['Close_log_diff'], lags=40, ax=ax)\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "model = ARIMA(df.Close, order=(1,1,0))\n",
        "model_fit = model.fit()\n",
        "model_fit.plot_diagnostics()"
      ],
      "metadata": {
        "id": "YWtQhrlvgzLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#costruzione di un modello che prevede il prezzo del giorno dopo\n",
        "TIMESTP = 2500 #per il train\n",
        "train, test = df.Close[:TIMESTP], df.Close[TIMESTP:]\n",
        "len(train), len(test)\n",
        "\n",
        "history = [x for x in train]\n",
        "y_hats = list() #nostre y\n",
        "ys = list() #y vere\n",
        "error_ls = list()\n",
        "\n",
        "print('Printing Predicted vs Expected Values...')\n",
        "print('\\n')\n",
        "for t in range(len(test)):\n",
        "  model = ARIMA(history, order=(1, 1, 0)).fit()\n",
        "\n",
        "  output = model.forecast()\n",
        "  pred_value = output[0]\n",
        "\n",
        "  original_value = test[t]\n",
        "  history.append(original_value)\n",
        "\n",
        "  #Calculating the error\n",
        "  err = ((abs(pred_value - original_value)) / original_value) * 100\n",
        "  error_ls.append(err)\n",
        "\n",
        "  y_hats.append(float(pred_value))\n",
        "  ys.append(float(original_value))\n",
        "\n",
        "print('\\n Mean Error in Predicting Test Case: %f ' %(sum(error_ls) / len(error_ls)), '%')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "test_day= [t for t in range(len(test))]\n",
        "labels = {'Original', 'Predicted'}\n",
        "plt.plot(test_day, y_hats, color='green')\n",
        "plt.plot(test_day, ys, color='orange')\n",
        "plt.title('Expected VS Predicted Forecasting')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.legend(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sz0Hb0fgmcCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image background remover\n",
        "\n",
        "Obiettivo: costruire immagini di facce umane senza sfondo.\n",
        "\n",
        "1. Carica un immagine di una faccia umana in posizione frontale\n",
        "2. Rintraccia il rettangolo contenente la faccia (haae cascades algorith)\n",
        "3. Applicare il grabCut algorith per individuare bg/fg e mask contenente l'oggetto\n",
        "4. Applica un bitwise_and sull'immagine originale per ottenere l'immagine con la sola faccia e nessuno sfondo."
      ],
      "metadata": {
        "id": "7x1T-sYOhvVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ProfAI/machine-learning-avanzato.git"
      ],
      "metadata": {
        "id": "lh4geiv3qFZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # per printare su una stessa cella"
      ],
      "metadata": {
        "id": "xDO7B51BqaYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_rectangle(img, start_point, end_point):\n",
        "  return cv2.rectangle(img,start_point, end_point, (255, 0, 255), 2)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('./machine-learning-avanzato/datasets/haarcascade_frontalface_alt.xml')"
      ],
      "metadata": {
        "id": "GaoCUy1prciX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Individuazione del rettangolo contenente la faccia\n",
        "img = cv2.imread('./machine-learning-avanzato/datasets/face.jpg')\n",
        "plt.imshow(img)\n",
        "\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #facciamo versione grayscale per semplificare e eliminare complessità della luminosità\n",
        "gray = cv2.equalizeHist(gray) #aumentiamo contrasto\n",
        "plt.imshow(gray, cmap='gray')"
      ],
      "metadata": {
        "id": "IlLfL2tjr8Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faces = face_cascade.detectMultiScale(gray, 1.25, 15) #impostiamo threshold\n",
        "len(faces) #lista di x, y, larghezza, altezza\n",
        "\n",
        "plt.imshow(draw_rectangle(img, (faces[0][0], faces[0][1]), (faces[0][0]+faces[0][2], faces[0][1]+faces[0][3])))"
      ],
      "metadata": {
        "id": "12F65qQWso2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ridefiniamo il tutto in maniera più analitica...\n",
        "img = cv2.imread('./machine-learning-avanzato/datasets/face.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "gray = cv2.equalizeHist(gray)\n",
        "faces = face_cascade.detectMultiScale(gray, 1.25, 15)\n",
        "if len(faces):\n",
        "  print('Face detected...')\n",
        "  for (x,y,w,h) in faces:\n",
        "    _ = draw_rectangle(img, (x, y), (x+w, y+h))\n",
        "_ = plt.imshow(img)\n",
        "\n"
      ],
      "metadata": {
        "id": "BWGTMqdQt-sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LDA e QDA"
      ],
      "metadata": {
        "id": "5bEplfaCx-ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. calcolo del vettore delle media d-dimensionale per la classi del dataset\n",
        "2. calcola le class scatter matrix\n",
        "3. calcolo autovettori e autovalori della matrice di covarianza\n",
        "4. ordina gli autovettori in ordine decrescente rispetto agli autovalori e sceglie i primi k autovettori con gli autovalori associati più grandi formando matrice dx k (le colonne sono autovettori)\n",
        "5. usa la mtrice degli autovettoi per trasformare i vettori dello spazio originario di dimensioni 1-d di dimensioni 1-k"
      ],
      "metadata": {
        "id": "RM1iDBg0yHq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # per printare su una stessa cella"
      ],
      "metadata": {
        "id": "Gd8O76zey3J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dict = {i :label for i, label in zip(range(4), (\n",
        "    'sepal length in cm',\n",
        "    'sepal width in cm',\n",
        "    'petal length in cm',\n",
        "    'petal width in cm',\n",
        "))}"
      ],
      "metadata": {
        "id": "ri7uoTQw8sm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
        "df = pd.read_csv(URL, header=None)\n",
        "\n",
        "df.columns = [el for el in sorted(feature_dict.values())] + ['class label']\n",
        "df.head()\n",
        "\n",
        "df['class label'].value_counts(dropna=False) #non ci sono valori NA"
      ],
      "metadata": {
        "id": "jM8Gssm-9Mvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing dei dati\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = df[df.columns[:-1]].values\n",
        "y = df['class label'].values\n",
        "\n",
        "enc = LabelEncoder()\n",
        "label_encoder = enc.fit(y)\n",
        "y = label_encoder.transform(y) + 1\n",
        "\n",
        "label_dict = {1: 'Setosa', 2: 'Versicolor', 3: 'Virginica'}"
      ],
      "metadata": {
        "id": "i3i7tI5z92H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#istogramma del dataset, si evince che le classi dividono in cluster differenziti il dataset\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))\n",
        "for ax, cnt in zip(axes.ravel(), range(4)):\n",
        "  #set bin sizes\n",
        "  min_b = math.floor(np.min(X[:cnt]))\n",
        "  max_b = math.floor(np.max(X[:cnt]))\n",
        "  bins = np.linspace(min_b, max_b)\n",
        "\n",
        "  #plot histogram\n",
        "  for lab, col in zip(range(4), ('blue', 'red', 'green')):\n",
        "    _ = ax.hist(X[y==lab, cnt],\n",
        "                color=col,\n",
        "                label=f'class {label_dict[lab]}',\n",
        "                bins=bins,\n",
        "                alpha=0.5)\n",
        "\n",
        "    ylims = ax.get_lim()\n",
        "\n",
        "    #annotation\n",
        "    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n",
        "    _= leg.get_frame().set_alpha(0.5)\n",
        "    _= ax.set_ylim(0, max(ylims) + 2)\n",
        "    _= ax.set_xlim(feature_dict[cnt])\n",
        "    _ = ax.set_title('Iris Histogram #%s ' %str(cnt + 1))\n",
        "\n",
        "    #hide axis ticks\n",
        "    _ = ax.tick_params(axis='both', which='bot', bottom='off', top='off',  labelBottom='on', left='off', right='off', labelleft='on')\n",
        "\n",
        "    #remove axis spines\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "\n",
        "  _ = axes[0][0].set_ylabel('count')\n",
        "  _ = axes[1][0].set_ylabel('count')\n",
        "\n",
        "  fig.tigh_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mEJK7f3U-Wr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. calcolo dei vettori medi per ogni classe (media di ogni feature associata alla classe)"
      ],
      "metadata": {
        "id": "EXtL7bJOHn3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "mean_vectors = []\n",
        "for cl in range(1,4):\n",
        "  mean_vectors.append(np.mean(X[y==cl], axis=0))\n",
        "  print(f'Mean Vector Class {cl}: {mean_vectors[-1]}')"
      ],
      "metadata": {
        "id": "3_kidy-3CWBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. calcolo matrice di scatter per la covarianza"
      ],
      "metadata": {
        "id": "gfzJbzDPHj4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "S_W = np.zeros((4,4)) #abbiamo 4 classi per 3 classi\n",
        "for cl, mv in zip(range(1,4), mean_vectors):\n",
        "  class_f = np.zeros((4,4))\n",
        "  for row in X[y==cl]:\n",
        "    row, mv = row.reshape(4,1), mv.reshape(4,1)\n",
        "    xx = (row-mv)\n",
        "    class_f += xx.dot(xx.T)\n",
        "  S_W += class_f\n",
        "  print(f'within-class Scatter Matrix: \\n {S_W}')\n",
        "\n",
        "overall_mean = np.mean(X, axis=0)\n",
        "overall_mean = overall_mean.reshape(4,1)\n",
        "S_B = np.zeros((4,4))\n",
        "for i, mean_vec in enumerate(mean_vectors):\n",
        "  N = len(X[y==i+1])\n",
        "  mean_vec = mean_vec.reshape(4,1) #make a column vector\n",
        "  xx = mean_vec - overall_mean\n",
        "  S_B += N * xx.dot(xx.T)\n",
        "\n",
        "print(f'between-class Scatter Matrix: \\n {S_B}')"
      ],
      "metadata": {
        "id": "G_vT3XwpDK36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. trova autovalori e autovettori\n",
        "\\begin{equation}\n",
        "\\mathbf{S}^{-1}_w \\mathbf{S}_B\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "ZDOABe9XGg4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "for i in range(len(eig_vals)):\n",
        "  eigvec_sc = eig_vecs[:, i].reshape(4,1)\n",
        "  print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
        "  print('Eigenvalue {}: \\n{}'.format(i+1, eig_vals[i].real))"
      ],
      "metadata": {
        "id": "aTn183avGHE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. ordina gli autovettori e seleziona sottoinsieme"
      ],
      "metadata": {
        "id": "30ooAUDKIS5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,1])  for i in range(len(eig_vals))]\n",
        "eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
        "eig_pairs\n",
        "\n",
        "#matrice con gli autovettori che selezioniamo\n",
        "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
        "print(f'Matrix W:\\n {W}')"
      ],
      "metadata": {
        "id": "59voUfNvGgK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. trasformiamo il dataset di partenza (vettori con dimensione 1-4) in vettori di dimensione 1,2...quindi lo proiettiamo in uno spazio a due dimensioni"
      ],
      "metadata": {
        "id": "Tqgr1mtMIUqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_lda = X.dot(eig_pairs[0][1])\n",
        "X_lda\n",
        "\n",
        "from matplotlib.pyplot import plt\n",
        "\n",
        "def plot_step_lda():\n",
        "  ax = plt.subplot(111)\n",
        "  for label, marker, color in zip(range(1,4), ('^', 's', 'o'), ('blue', 'red', 'green')):\n",
        "    plt.scatter(x=X_lda[:,0].real[y==label],\n",
        "                y=X_lda[:,1].real[y==label],\n",
        "                marker = marker,\n",
        "                color = color,\n",
        "                alpha = 0.5,\n",
        "                label = label_dict[label])\n",
        "  plt.xlabel('LD1')\n",
        "  plt.ylabel('LD2')\n",
        "\n",
        "  leg = plt.legend(loc='upper right', fancybox=True)\n",
        "  _= leg.get_frame().set_alpha(0.5)\n",
        "  plt.title('LDA: Iris Projection onto the first 2 linear discriminants')\n",
        "\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "plot_step_lda()"
      ],
      "metadata": {
        "id": "MoJZ9UEwIjJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sistemi di raccomandazione\n",
        "\n",
        "Esempio Content-Based"
      ],
      "metadata": {
        "id": "LKnIn75enHlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" # per printare su una stessa cella\n",
        "\n",
        "!git clone https://github.com/ProfAI/machine-learning-avanzato.git"
      ],
      "metadata": {
        "id": "MzqrDAUsnUy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/machine-learning-avanzato/datasets/imdb_top_1000.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dR1UtWQtnwJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[:, [\"Series_Title\", \"Overview\", \"Genre\"]]\n",
        "df.head() #ci focalizziamo tra tre features"
      ],
      "metadata": {
        "id": "WcYuBTIun9mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calcoliamo gli embedded dei contenuti, ovvero una rappresentazione vettoriale\n",
        "#lavoriamo sulla overview\n",
        "\n",
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "\n",
        "X = df['Overview'].values\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(X)\n",
        "embeddings.shape\n",
        "\n",
        "score = np.sum(np.dot(embeddings[0,:], embeddings[0,:].T) / (np.linalg.norm(embeddings[0,:,])*np.linalg.norm(embeddings[0,:,])))\n",
        "score\n",
        "\n",
        "cos_scores = util.cos_sim(embeddings, embeddings)\n",
        "cos_scores.shape\n",
        "cos_scores[0,1]\n",
        "\n",
        "pairs = []\n",
        "for i in range(len(cos_scores)-1):\n",
        "  for j in range(i+1, len(cos_scores)):\n",
        "    pairs.append({'index': (i,j), 'score': cos_scores[i,j]})\n",
        "\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "#verifichiamo sogmiglianza tra le frasi\n",
        "for pair in pairs[:10]:\n",
        "  i,j = pair['index']\n",
        "  sent1, sent2 = X[i], X[j]\n",
        "  score = pair['score']\n",
        "  print(f'{sent1}|{sent2}')\n",
        "  print(f'Similarity Score {score}')"
      ],
      "metadata": {
        "id": "uO4qIkHPoNM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}