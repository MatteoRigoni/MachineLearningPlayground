{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5G9Rv9echgLJ"
      ],
      "mount_file_id": "1m26j6sVUnDJ5ADTF-6IYjVmRlJ8saxbY",
      "authorship_tag": "ABX9TyMEqmKBRL8QbEqWfQmdWuTx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatteoRigoni/MachineLearningPlayground/blob/master/BaseMachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tecniche machine learning\n",
        "\n",
        "> Apprendimento supervisionato (abbiamo input e output)\n",
        "*  Regressione (l'output è un valore continuo, una quantità)\n",
        "*  Classificazione (l'output è un valore discreto)\n",
        "\n",
        "> Apprendimento non supervisionato (abbiamo solo input)\n",
        "*  Associations (trovare regole che descrivono una porzione grande di dati)\n",
        "*  Clustering (si raggruppano i dati per proprietà comuni)\n",
        "\n",
        "> Apprendimento semi-supervisionato\n",
        "*  Utilizzo di molti esempi senza label e pochi esempi con label\n",
        "\n",
        "> Apprendimento per rinforzo\n",
        "*  Si basa sulla realizzazione di **agenti intelligenti** in grado di prendere decisioni ed eseguire **azioni** in uno specifico **ambiente** al fine di massimizzare un **reward** e raggiungere un **obiettivo**\n"
      ],
      "metadata": {
        "id": "OHUHm3PJtHZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "5G9Rv9echgLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tDif3uLChfDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/\""
      ],
      "metadata": {
        "id": "-INJd2WSkzus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*csv con indicazione colonna indice*\n",
        "\n"
      ],
      "metadata": {
        "id": "kio8TbeVon7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL + \"shirts_example.csv\", index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "P8oJBZjwl8y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*tsv con indicazione colonna indice*"
      ],
      "metadata": {
        "id": "Xc6rLMgLpL71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL + \"shirts_example.tsv\", index_col=0, sep=\"\\t\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "MTHhdHwBlAN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*json strutturato in formato tabellare*"
      ],
      "metadata": {
        "id": "wg8hAv0QpR5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(BASE_URL + \"shirts_example.json\")\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "dvdVx70Qmc1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*xml con elementi \"row\" indicati esplicitamente*"
      ],
      "metadata": {
        "id": "78WG-TIspStR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_xml(BASE_URL + \"shirts_example.xml\", xpath='.//row')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cRMqAOwbm-P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*html con unica table con indicazione indice e riga di header*"
      ],
      "metadata": {
        "id": "uZsYOPFCpTIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_html(BASE_URL + \"shirts_example.html\", index_col=0, header = 0)\n",
        "type(df) # lista di tutte le tabelle\n",
        "type(df[0])\n",
        "df[0].head()"
      ],
      "metadata": {
        "id": "l8aGDG9ioYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*excel office oppure openoffice*"
      ],
      "metadata": {
        "id": "4ikr1iicqJ9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install odfpy"
      ],
      "metadata": {
        "id": "0rjNDOjRq2UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(BASE_URL + \"shirts_example.ods\", index_col=0) #header=None per non usare prima riga come header\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ZHdw_4i5qMFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "R5qr-t-hrcn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9E_ew0lN0ZcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/\""
      ],
      "metadata": {
        "id": "27HEtfYx0ihz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Ordinal encoding su variabile qualitativa*\n",
        "\n",
        "Assegnazione di valore numero ordinato da 1"
      ],
      "metadata": {
        "id": "A-B4vmp_quo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  pandas"
      ],
      "metadata": {
        "id": "6ygpdi4C3z43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"shirts.csv\", index_col=0)\n",
        "\n",
        "size_mapping = {\"S\":1, \"M\":2, \"L\":3, \"XL\":4}\n",
        "type(df[\"taglia\"] ) #series of dataframe\n",
        "df[\"taglia\"] = df[\"taglia\"].map(size_mapping)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OeDl-wIl0-D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  numpy"
      ],
      "metadata": {
        "id": "cTLkpK794CD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"shirts.csv\", index_col=0)\n",
        "\n",
        "X = df.values #array numpy multidimensionale (100x3)\n",
        "X.shape\n",
        "\n",
        "size_mapping = {\"S\":1, \"M\":2, \"L\":3, \"XL\":4}\n",
        "fmap = np.vectorize(lambda t:size_mapping[t]) #funzione vettorizzata da applicare ad array numpy\n",
        "X[:,0] = fmap(X[:,0])\n",
        "X[:5,:]"
      ],
      "metadata": {
        "id": "5bxeOvu12RZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*One hot encoding su variabili categoriche*\n",
        "\n",
        "Aggiunta di colonna true/false per ogni possibile valore"
      ],
      "metadata": {
        "id": "MraNbxuw4OJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  pandas"
      ],
      "metadata": {
        "id": "PHAaihks4zAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"shirts.csv\", index_col=0)\n",
        "\n",
        "df = pd.get_dummies(df, columns=[\"colore\"], prefix=\"color\", prefix_sep=\"-\") # \"nomeColonnaOriginale_Classe\" di default\n",
        "df.head()"
      ],
      "metadata": {
        "id": "E5Q8CPeo4UFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  numpy con scikit-learn"
      ],
      "metadata": {
        "id": "n1-ZK30H4zrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "df = pd.read_csv(BASE_URL+\"shirts.csv\", index_col=0)\n",
        "\n",
        "X = df.values #array numpy multidimensionale (100x3)\n",
        "X.shape\n",
        "\n",
        "transf = ColumnTransformer(\n",
        "    [\n",
        "        (\"ohe\", OneHotEncoder(), [1]) #tupla identificativa, ultimo parametro è la lista delle colonne\n",
        "    ],\n",
        "    remainder = \"passthrough\" #per mantenere colonne non modificate\n",
        ")\n",
        "\n",
        "X = transf.fit_transform(X) #array numpy trasformato\n",
        "X"
      ],
      "metadata": {
        "id": "BGBFpmxw41rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Label encoding*\n",
        "\n",
        "Codificare variabile target che si presenta come stringa"
      ],
      "metadata": {
        "id": "rqR_JG7i7XZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   pandas (come ordinal encoding sopra, assegnando valori 1 e 0)"
      ],
      "metadata": {
        "id": "TDaikDz_7zIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"shirts_sold.csv\", index_col=0)\n",
        "\n",
        "size_mapping = {\"SI\":1, \"NO\":0}\n",
        "df[\"venduta\"] = df[\"venduta\"].map(size_mapping)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jM8mjrAh9Lm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  numpy"
      ],
      "metadata": {
        "id": "eVs3hluD7_Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"shirts_sold.csv\", index_col=0)\n",
        "df.head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"venduta\"] = le.fit_transform(df[\"venduta\"])\n",
        "print(df.head())\n",
        "\n",
        "le.classes_ #classi usate\n",
        "\n",
        "# è possibile applicare trasformazione inversa...\n",
        "y = [0,1,0,0,1]\n",
        "y = le.inverse_transform(y)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "0ZSLyp_7489A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Drop per dati mancanti*"
      ],
      "metadata": {
        "id": "XWglxoDn8L7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"iris_missing.csv\", index_col=0)\n",
        "df.head()\n",
        "\n",
        "df.count() # numero colonne mancano su sepal_width e petal_width\n",
        "print(df.isna().sum()) # numero valori NA per colonna\n",
        "print(df.shape) # numero osservazioni iniziale\n",
        "\n",
        "#rimozione per riga...\n",
        "df_drop = df.copy()\n",
        "df_drop = df_drop.dropna()\n",
        "df_drop.head()\n",
        "print(df_drop.shape)\n",
        "\n",
        "# - usare \"how\" = all per rimuovere righe con tutte colonne vuote...\n",
        "# - usare \"subset\" = ... per rimuovere righe quella colonna vuote...\n",
        "df_drop = df_drop.dropna(subset=[\"sepal_width\"])\n",
        "\n",
        "#rimozione per colonna, possibile indicare soglia minima...\n",
        "df_drop = df.copy()\n",
        "df_drop = df_drop.dropna(axis=1, thresh=145)\n",
        "print(df_drop.shape)\n",
        "\n",
        "# - per calcolare percentuale al 90% del numero osservazioni:\n",
        "thresh = int(df.shape[0]*0.9)"
      ],
      "metadata": {
        "id": "TRYXfs2r92iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Imputazione dei dati feature mancanti, cioè sostituire NA (media, mediana, moda)*"
      ],
      "metadata": {
        "id": "MtezqQTS91XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  pandas (su singola colonna)"
      ],
      "metadata": {
        "id": "3dyQp_DWAwFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"iris_missing.csv\", index_col=0)\n",
        "df.head()\n",
        "\n",
        "df_imp = df.copy()\n",
        "\n",
        "col = \"sepal_width\"\n",
        "replace_with = round(df_imp[col].mean(), 1)\n",
        "# - si poteva fare anche con la median\n",
        "# - si poteva fare anche con la moda, prendendo il più frequente\n",
        "#   replace_with = round(df_imp[col].mode()[0], 1)\n",
        "\n",
        "df_imp[col] = df_imp[col].fillna(replace_with)\n",
        "df_imp.head()\n",
        "print(df_imp.isna().sum()) # ora gli NA sono zero per la colonna"
      ],
      "metadata": {
        "id": "bJoHTnwBAt1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  pandas (su più colonne)"
      ],
      "metadata": {
        "id": "U50mgSdgCmHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"iris_missing.csv\", index_col=0)\n",
        "df.head()\n",
        "\n",
        "df_imp = df.copy()\n",
        "\n",
        "replace_with = round(df_imp.mean(numeric_only=True), 1) #series con valori medi per colonna\n",
        "df_imp = df_imp.fillna(replace_with)\n",
        "df_imp.head()\n",
        "print(df_imp.isna().sum()) # tutti gli NA sono zero"
      ],
      "metadata": {
        "id": "rFATylf1B3hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  numpy e scikit-learn"
      ],
      "metadata": {
        "id": "Vkvufb_lDAXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df = pd.read_csv(BASE_URL+\"iris_missing.csv\", index_col=0)\n",
        "\n",
        "X = df.drop(\"species\", axis=1).values #si rimuove variabile target, che non va adeguata..\n",
        "X.shape\n",
        "\n",
        "imp = SimpleImputer(strategy=\"mean\")\n",
        "X_imp = imp.fit_transform(X)\n",
        "X_imp[:5]\n",
        "np.isnan(X).sum(axis=0)\n",
        "np.isnan(X_imp).sum(axis=0) #valori NaN per asse colonna"
      ],
      "metadata": {
        "id": "d6WCTEIKDHFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Normalizzazione*\n",
        "\n",
        "x-min(x) / max(x) - min(x)"
      ],
      "metadata": {
        "id": "9BwyruQFDov6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  pandas"
      ],
      "metadata": {
        "id": "ahtWGv2mGtZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"wine.csv\", usecols=[0,1,7]) #usiamo solo alcune featues per l'esercizio\n",
        "df.head()\n",
        "\n",
        "df_norm = df.copy()\n",
        "features = [\"alcol\", \"flavonoidi\"] #escludiamo il target che è classe, che non ha senso normalizzare\n",
        "to_norm = df_norm[features]\n",
        "df_norm[features] = (to_norm-to_norm.min()) / (to_norm.max()-to_norm.min())\n",
        "df_norm.head() #così abbiamo tutto tra min 0 e max 1"
      ],
      "metadata": {
        "id": "s8G8MRdFEnUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  numpy e scikit-learn"
      ],
      "metadata": {
        "id": "wwFtJkdAHenB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "df = pd.read_csv(BASE_URL+\"wine.csv\", usecols=[0,1,7]) #usiamo solo alcune featues per l'esercizio\n",
        "X = df.drop(\"classe\", axis=1).values #rimuoviamo colonna target\n",
        "X.shape\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "X_norm = mms.fit_transform(X)\n",
        "X_norm[:5] #così abbiamo tutto tra min 0 e max 1"
      ],
      "metadata": {
        "id": "O33ZygUzHXde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Standardizzazione*\n",
        "\n",
        "x-mean(x) / sd(x)"
      ],
      "metadata": {
        "id": "rI7rCU21IVK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  pandas"
      ],
      "metadata": {
        "id": "QORIsQMiIas9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(BASE_URL+\"wine.csv\", usecols=[0,1,7]) #usiamo solo alcune featues per l'esercizio\n",
        "df.head()\n",
        "\n",
        "df_std = df.copy()\n",
        "features = [\"alcol\", \"flavonoidi\"] #escludiamo il target che è classe, che non ha senso standardizzare\n",
        "to_std = df_std[features]\n",
        "\n",
        "df_std[features] = (to_std - to_std.mean()) / to_std.std(ddof=0) #ddof per non fare -1 (std campionaria) ma della popolazione (come numpy)\n",
        "df_std.head() #infatti la media è quasi uguale a zero e la deviazione standard prossima a 1"
      ],
      "metadata": {
        "id": "TmV7yw6sH0tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  scikit-learn"
      ],
      "metadata": {
        "id": "w9MmJEMpJQoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv(BASE_URL+\"wine.csv\", usecols=[0,1,7]) #usiamo solo alcune featues per l'esercizio\n",
        "X = df.drop(\"classe\", axis=1).values #rimuoviamo colonna target\n",
        "X.shape\n",
        "\n",
        "ss = StandardScaler()\n",
        "X_std = ss.fit_transform(X)\n",
        "X_std[:5]\n",
        "print(X_std.mean()) # prossimo zero\n",
        "print(X_std.std()) # prossimo a 1"
      ],
      "metadata": {
        "id": "3qK-F-yZJUri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMwv43jVI9Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset + Data Preprocessing (esercizio)"
      ],
      "metadata": {
        "id": "O2Q6Xr4mLOOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/housing_dirty.csv\"\n",
        "\n",
        "df = pd.read_csv(DATASET_URL, index_col=0)\n",
        "df.head()\n",
        "\n",
        "### numero righe/colonne dataset\n",
        "df.shape\n",
        "\n",
        "### tipologia variabile e distribuzione\n",
        "df.info()\n",
        "# per quantitative...\n",
        "df.describe()\n",
        "# per qualitative numero occorrenze...\n",
        "df[\"CRIM\"].value_counts()\n",
        "df[\"CHAS\"].value_counts()\n",
        "\n",
        "### valori mancanti per colonna\n",
        "df.count()\n",
        "df.isna().sum()\n",
        "\n",
        "### rimuovi le colonne con il 30% di dati mancanti\n",
        "df.count()<df.shape[0]*0.7 #df = df.drop(\"LSTAT\", axis=1)\n",
        "thresh = df.shape[0]*0.7\n",
        "df = df.dropna(thresh=thresh, axis=1)\n",
        "df.head()\n",
        "\n",
        "### rimuovi le righe con il 75% di dati mancanti\n",
        "df = df.dropna(thresh=df.shape[1]*0.75)\n",
        "df.shape\n",
        "\n",
        "### rimuovi righe con campo mancante\n",
        "df = df.dropna(subset=[\"PRICE\"])\n",
        "df[\"PRICE\"].isna().sum()\n",
        "df.shape\n",
        "\n",
        "### imputazione per valore medio su variabili quantitative\n",
        "cols = df.columns\n",
        "cols = cols.drop([\"CRIM\", \"CHAS\"])\n",
        "replace_with = df[cols].mean()\n",
        "df = df.fillna(replace_with)\n",
        "df.isna().sum()\n",
        "\n",
        "### codifica di variabili qualitative\n",
        "mapping = {\"LOW\":1, \"MODERATE\":2, \"HIGH\":3, \"VERY HIGH\":4}\n",
        "df[\"CRIM\"] = df[\"CRIM\"].map(lambda t: mapping[t])\n",
        "mapping = {\"YES\":1, \"NO\":0}\n",
        "df[\"CHAS\"] = df[\"CHAS\"].map(lambda t: mapping[t])\n",
        "df.head()\n",
        "\n",
        "### imputazione con moda su variabili qualitative\n",
        "cols = [\"CRIM\", \"CHAS\"]\n",
        "replace_with = df[cols].mode()\n",
        "df[cols] = df[cols].fillna(replace_with)\n",
        "df.head()\n",
        "\n",
        "### standardizzazione\n",
        "features = df.columns.drop(\"PRICE\")\n",
        "df[features] = (df[features] - df[features].mean())  / df[features].std()\n",
        "df[features].mean()\n",
        "df[features].std()\n",
        "\n",
        "### salva dataframe in tsv\n",
        "df.to_csv(\"housing_cleaned.tsv\", sep=\"\\t\")"
      ],
      "metadata": {
        "id": "EblyXvFHLT5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regressione lineare"
      ],
      "metadata": {
        "id": "5Y3_0hajhFMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Regressione lineare semplice*"
      ],
      "metadata": {
        "id": "d02bp_cYhen_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  numpy"
      ],
      "metadata": {
        "id": "fCgY3GzBi0uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "aAM9rEEnMQ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "  coef = None #peso\n",
        "  intercept = None #bias\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    x_sum = x.sum()\n",
        "    y_sum = y.sum()\n",
        "    xy_sum = (x*y).sum()\n",
        "    x2_sum = (x*x).sum()\n",
        "    n = y.shape[0]\n",
        "\n",
        "    self.coef = (n*xy_sum - x_sum*y_sum) / (n*x2_sum - x_sum*x_sum)\n",
        "    self.intercept = (y_sum-self.coef * x_sum) / n\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.coef*x + self.intercept"
      ],
      "metadata": {
        "id": "xMegUUVlhVgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([80,150,30,50,120,60,110,110]) #metri quadri\n",
        "y_train = np.array([16,30,12,10,24,18,20,25]) #valore casa x10000$\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "print(lr.coef)\n",
        "print(lr.intercept)"
      ],
      "metadata": {
        "id": "hWrp5QxQjAxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.predict(70) # esempio di predizione\n",
        "y_pred = lr.predict(x_train)"
      ],
      "metadata": {
        "id": "bx6jCcFYlr8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Funzioni di costo (valore minore  = maggiore bontà del modello)\n",
        "*  Funzioni di scoring (valore maggiore  = maggiore bontà del modello)"
      ],
      "metadata": {
        "id": "t2ae3XAhmoFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[COST] errore assoluto medio (MAE), indica di quanto più o meno sbaglia il modello\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "  return np.abs(y_true - y_pred).sum() / y_true.shape[0]\n",
        "\n",
        "#[COST] errore quadratico medio (MSE)\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.square(y_true - y_pred).sum() / y_true.shape[0]\n",
        "\n",
        "#[COST] radice dell'errore quadratico medio (RMSE)\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "  return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "#[SCORE] R sqared R2\n",
        "def rss(y_true, y_pred): #Somma quadrati residui\n",
        "  return np.power(y_true-y_pred, 2).sum() #MSE senza divisione per R\n",
        "def sst(y_true): #Somma quadrati totali\n",
        "  return np.power(y_true-y_true.mean(), 2).sum()\n",
        "def r2_score(y_true, y_pred):\n",
        "  return 1 - rss(y_true, y_pred) / sst(y_true)"
      ],
      "metadata": {
        "id": "16-kpT1Ulutf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_error(y_train, y_pred) #stessa scala dei dati"
      ],
      "metadata": {
        "id": "7i38AF8xoRn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#entrambi sempre maggiori del MSE\n",
        "mean_squared_error(y_train, y_pred) #scala diversa dai dati, da riportare con radice quadrata\n",
        "root_mean_squared_error(y_train, y_pred) #qui sono penalizzati errori più grandi del modello"
      ],
      "metadata": {
        "id": "0SwMAmPwrDxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.3 - 0.5 = scarso\n",
        "# 0.5 - 0.7 = discreto\n",
        "# 0.7 - 0.9 = buono\n",
        "# 0.9 - 1.0 = ottimo\n",
        "r2_score(y_train, y_pred)"
      ],
      "metadata": {
        "id": "SnOTS6DC63dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Rappresentazione grafica della predizione rispetto ai valori reali"
      ],
      "metadata": {
        "id": "rLZYPFh6kc3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pDFvx2hHkJd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figsize=(10,8)\n",
        "\n",
        "x_line = np.arange(1, 150) # valori da 0 a 150\n",
        "y_line = lr.predict(x_line)\n",
        "\n",
        "plt.grid()\n",
        "plt.scatter(x_train, y_train, c=\"green\")\n",
        "plt.plot(x_line, y_line, c=\"red\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M0bCdW5KkOjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  scikit-learn"
      ],
      "metadata": {
        "id": "vp2KlFQak7Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_errore, mean_absolute_errore, r2_score\n",
        "\n",
        "x_train = np.array([[80],[150],[30],[50],[120],[60],[110],[110]]) #metri quadri (anche per una feature, bidimensionale in scikit-learn)\n",
        "y_train = np.array([16,30,12,10,24,18,20,25]) #valore casa x10000$\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "lr.coef_\n",
        "lr.intercept_\n",
        "\n",
        "y_pred = lr.predict(x_train)\n",
        "\n",
        "mean_absolute_error(y_train, y_pred) #MAE\n",
        "mean_squared_error(y_train, y_pred) #MSE\n",
        "sqrt(mean_squared_error(y_train, y_pred)) #RMSE\n",
        "r2_score(y_train, y_pred) #R2"
      ],
      "metadata": {
        "id": "OvJ3oJUX_ADh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Regressione lineare multipla*\n",
        "\n",
        "Con più variabili il modello diventa più accurato"
      ],
      "metadata": {
        "id": "OlUetme9AN2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([[80,1995],[150,1995],[30,2008],[50,1996],[120,1994],[60,2006],[110,1989],[110,2000]]) #metri quadri e anno costruzione\n",
        "y_train = np.array([16,30,12,10,24,18,20,25]) #valore casa x10000$\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_train)\n",
        "\n",
        "mean_absolute_error(y_train, y_pred) #MAE\n",
        "mean_squared_error(y_train, y_pred) #MSE\n",
        "sqrt(mean_squared_error(y_train, y_pred)) #RMSE\n",
        "r2_score(y_train, y_pred) #R2"
      ],
      "metadata": {
        "id": "kSqOpDOsARMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Matrice di correlazione*\n",
        "\n",
        "Se abbiamo tante features non possiamo usarle tutte, ma dobbiamo prendere quella più significative.\n",
        "\n",
        "Se due features hanno alta correlazione una è superflua in quanto si determinano a vicenda"
      ],
      "metadata": {
        "id": "OsB17ndjA89K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb"
      ],
      "metadata": {
        "id": "vTrb5y6UBE5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[80,1995,16],[150,1995,30],[30,2008,12],[50,1996,10],[120,1994,24],[60,2006,18],[110,1989,20],[110,2000,25]] #metri quadri, anno costruzione, target(prezzo)\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Dimensione\", \"Anno\", \"Valore\"])\n",
        "df.corr() #indice correlazione di Pearson di default\n",
        "sb.heatmap(df.corr(), annot=True) #rappresentazione visiva della correlazione\n",
        "\n",
        "# se avevamo un array numpy e non un dataset avremmo potuto passare i nomi delle colonne\n",
        "data = df.corr().values\n",
        "columns = [\"Dimensione\", \"Anno\", \"Valore\"]\n",
        "sb.heatmap(data, annot=True, xticklabels=columns, yticklabels=columns)"
      ],
      "metadata": {
        "id": "QTH0bNeOBG1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Regressione lineare polinominale*\n",
        "\n",
        "In questo caso dopo il grado 10 la complessità aumenta e non è più efficace.\n",
        "\n",
        "Con troppi gradi però rischia di essere però troppo legato al dataset di partenza."
      ],
      "metadata": {
        "id": "-8nMbRjpDwvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([[80],[150],[30],[50],[120],[60],[110],[110]]) #metri quadri (anche per una feature, bidimensionale in scikit-learn)\n",
        "y_train = np.array([16,30,12,10,24,18,20,25]) #valore casa x10000$\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "poly = PolynomialFeatures(2, include_bias=True) #2 gradi\n",
        "poly.fit(x_train)\n",
        "\n",
        "x_train_poly = poly.transform(x_train)\n",
        "x_train_poly.shape #1° colonna sempre il bias uguale a 1, poi la x e la x^2\n",
        "\n",
        "#verifica del modello su più gradi\n",
        "degrees = [2, 3, 4, 5, 10, 20, 50, 100]\n",
        "\n",
        "x_line = np.arange(1,150) #x su cui facciamo la previsione\n",
        "\n",
        "for d in degrees:\n",
        "  poly = PolynomialFeatures(d)\n",
        "  x_train_poly = poly.fit_transform(x_train)\n",
        "\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x_train_poly, y_train)\n",
        "\n",
        "  y_pred = lr.predict(x_train_poly)\n",
        "  mse = mean_squared_error(y_train, y_pred)\n",
        "  r2 = r2_score(y_train, y_pred)\n",
        "\n",
        "  #su x line dobbiamo creare feature polinomiali, i dati da prevedere devono avere stesso trattamento di quelli dell'addestramento\n",
        "  x_line_poly = poly.transform(x_line.reshape(-1,1)) #deve essere bidimensionale per scikit-learn (reshape)\n",
        "  y_line = lr.predict(x_line_poly)\n",
        "\n",
        "  plt.figure(figsize=(6,4))\n",
        "  plt.grid()\n",
        "\n",
        "  plt.scatter(x_train, y_train, c=\"green\")\n",
        "  plt.plot(x_line, y_line, c=\"red\")\n",
        "\n",
        "  plt.ylim(0,30)\n",
        "  plt.text(0, 28, f\"Degree: {d}\")\n",
        "  plt.text(125, 5, f\"MSE: {mse:.2f}\")\n",
        "  plt.text(125, 2, f\"R2: {r2:.2f}\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "s2IBhpXOD3ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Regressione lineare polinominale multidimensionale*"
      ],
      "metadata": {
        "id": "Hscck63LJa5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([[80,1995],[150,1995],[30,2008],[5,19960],[120,1994],[60,2006],[110,1989],[110,2000]]) #metri quadri, anno\n",
        "y_train = np.array([16,30,12,10,24,18,20,25]) #valore casa x10000$\n",
        "\n",
        "poly = PolynomialFeatures()\n",
        "\n",
        "x_train_poly = poly.fit_transform(x_train)\n",
        "x_train_poly.shape\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train_poly, y_train)\n",
        "\n",
        "y_pred = lr.predict(x_train_poly)\n",
        "\n",
        "#valori molto alti, probabilmente dovuti a overfitting!\n",
        "mean_squared_error(y_train, y_pred)\n",
        "r2_score(y_train, y_pred)"
      ],
      "metadata": {
        "id": "bZ9sH9k2JdEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regressione lineare (esercizio)"
      ],
      "metadata": {
        "id": "mccYKZ97L4G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/housing.csv\"\n",
        "BASE_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/\"\n",
        "\n",
        "df = pd.read_csv(DATASET_URL, index_col=0)\n",
        "df.head()\n",
        "\n",
        "### 1) matrice correlazione\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(df.corr(), annot=True)\n",
        "#spiccano ad esempio LSTAT (tasso povertà) e RM (numero stanze) relativi al valore predetto PRICE\n",
        "#si notano poi relazioni forti anche tra alcune features\n",
        "\n",
        "### 2) modello regressione lineare sulla principale feature (tasso povertà)\n",
        "def evaluate(model, dataset):\n",
        "  x, y = dataset\n",
        "  y_pred = model.predict(x)\n",
        "  print(f\"MSE: {mean_squared_error(y, y_pred):.2f}\")\n",
        "  print(f\"R2 {r2_score(y, y_pred):.2f}\")\n",
        "\n",
        "x = df[[\"LSTAT\"]].values #doppia parentesi, deve essere bidimensionale per scikit-learn\n",
        "y = df[\"PRICE\"].values\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x,y)\n",
        "\n",
        "#MSE alto da raffrontare coi valori di y\n",
        "evaluate(lr, (x,y))\n",
        "y.min()\n",
        "y.max()\n",
        "y.mean()\n",
        "#il modello non è buono!\n",
        "\n",
        "### 3) modello regressione lineare multipla con 2 variabili più significative\n",
        "#si provano LSTAT e RM, che però sono molto collegate anche tra loro, quindi al massimo si passa alla successiva\n",
        "x = df[[\"LSTAT\", \"RM\"]].values\n",
        "lr = LinearRegression()\n",
        "lr.fit(x,y)\n",
        "evaluate(lr, (x,y))\n",
        "#il modello migliora ma di poco...\n",
        "\n",
        "x = df[[\"LSTAT\", \"PTRATIO\"]].values\n",
        "lr = LinearRegression()\n",
        "lr.fit(x,y)\n",
        "evaluate(lr, (x,y))\n",
        "#il modello non migliora neanche così\n",
        "\n",
        "### 4) modello con terza variabile, provando polinomiale senza superare grado 5 con o senza bias\n",
        "x = df[[\"LSTAT\", \"RM\", \"PTRATIO\"]].values\n",
        "\n",
        "for i in range(1,6):\n",
        "  poly = PolynomialFeatures(i, include_bias=False) #si prova con o senza bias\n",
        "  x_poly = poly.fit_transform(x)\n",
        "\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x_poly,y)\n",
        "\n",
        "  print(f\"Polinomio di grado: {i} con bias\")\n",
        "  evaluate(lr, (x_poly, y))\n",
        "  print(f\"--------------------------------\")\n",
        "\n",
        "### 5) modello regressione lineare con tutte le variabili\n",
        "x = df.drop(\"PRICE\", axis=1).values #togliamo quindi solo il target\n",
        "y = df[\"PRICE\"].values\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x,y)\n",
        "evaluate(lr, (x,y))\n",
        "#modello buono 0.7\n",
        "\n",
        "### 6/7) normalizzazione/standardizzazione, cambia qualcosa?\n",
        "mms = MinMaxScaler()\n",
        "x_norm = mms.fit_transform(x)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_norm,y)\n",
        "evaluate(lr, (x_norm,y))\n",
        "#--------------------------\n",
        "sc = StandardScaler()\n",
        "x_std = sc.fit_transform(x)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_std,y)\n",
        "evaluate(lr, (x_std,y))\n",
        "#non cambia nulla sul metodo dei minimi quadrati\n",
        "#hanno senso quando c'è un processo di ottimizzazione iterativo\n",
        "\n",
        "### 8/9) utilizzo modello su un file csv e salvataggio risultato\n",
        "x = df.drop(\"PRICE\", axis=1).values\n",
        "y = df[\"PRICE\"].values\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x,y)\n",
        "\n",
        "df_pred = pd.read_csv(BASE_URL + \"housing_predict.csv\")\n",
        "df_pred.head()\n",
        "\n",
        "#droppiamo owner che non ha importanza, è un codice identificativo, non ripetuto senza informazioni utili\n",
        "x_pred = df_pred.drop(\"OWNER\", axis=1).values\n",
        "y_pred = lr.predict(x_pred)\n",
        "\n",
        "df_result = pd.DataFrame(\n",
        "    {\n",
        "      \"owner\":df_pred[\"OWNER\"].values,\n",
        "      \"estimated price\":y_pred,\n",
        "    }\n",
        ")\n",
        "df_result.to_excel(\"housing_estimate.xlsx\", index=False) #salviamo rimuovendo colonna di indice"
      ],
      "metadata": {
        "id": "kllFosPhL6fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting"
      ],
      "metadata": {
        "id": "wzwFcb0bLnrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "#generazione di un dataset affetto da ovefitting, ad esempio con n variabili = n osservazioni\n",
        "x, y = make_regression(\n",
        "    n_samples = 100,\n",
        "    n_features = 100,\n",
        "    n_informative = 10,\n",
        "    n_targets = 1,\n",
        "    random_state = RANDOM_SEED\n",
        ")"
      ],
      "metadata": {
        "id": "KUt73hnSLpYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hold out*\n",
        "\n",
        "Mettiamo una parte di dati da parte per il test"
      ],
      "metadata": {
        "id": "tjKacTk9MNXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# impostiamo un set di test del 30%, se ho molti dati posso ridurlo..\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=RANDOM_SEED)\n",
        "x_train.shape #70\n",
        "x_test.shape #30\n",
        "\n",
        "#standardizziamo i dati\n",
        "ss = StandardScaler()\n",
        "x_train = ss.fit_transform(x_train)\n",
        "x_test = ss.transform(x_test)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "  x, y = dataset\n",
        "  y_pred = model.predict(x)\n",
        "  print(f\"MSE: {mean_squared_error(y, y_pred):.3f}\")\n",
        "  print(f\"R2 {r2_score(y, y_pred):.3f}\")\n",
        "\n",
        "evaluate_model(lr, (x_train, y_train)) # r2 = 1, perfetto, da vedere però sul test set...\n",
        "evaluate_model(lr, (x_test, y_test)) # disastro, r2 = 0.4"
      ],
      "metadata": {
        "id": "yF_ppuncMRmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Cross validation*"
      ],
      "metadata": {
        "id": "S1eNgKwOPdDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lr = LinearRegression()\n",
        "score = cross_val_score(lr, x, y, cv=5, scoring=\"r2\") #numero batch = 5\n",
        "score # array con r2 per ognuno dei 5 modelli\n",
        "score.mean() #0.69\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "lr = LinearRegression()\n",
        "cv_result = cross_validate(lr, x, y, cv=5, return_train_score=True) # risultato anche addrestamento oltre che test\n",
        "cv_result[\"train_score\"].mean() # media di r2 a 1, caso overfitting\n",
        "cv_result[\"test_score\"].mean() # qui otteniamo 0.69, confermiamo overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "vnWdCMN3PgAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sopra non abbiamo fatto standardizzazione, perchè andrebbe fatto dentro i metodi di scikit-learn, dobbiamo riscriverci la funzione"
      ],
      "metadata": {
        "id": "la4Y_PvYRWRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "kf.get_n_splits(x) # 5 batch\n",
        "\n",
        "train_score = []\n",
        "test_score = []\n",
        "\n",
        "for train_index, test_index in kf.split(x):\n",
        "  x_train, x_test = x[train_index], x[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "  #ora possiamo standardizzare...\n",
        "  ss = StandardScaler()\n",
        "  x_train = ss.fit_transform(x_train)\n",
        "  x_test = ss.transform(x_test)\n",
        "\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x_train, y_train)\n",
        "\n",
        "  r2_train = r2_score(y_train, lr.predict(x_train))\n",
        "  r2_test = r2_score(y_test, lr.predict(x_test))\n",
        "\n",
        "  train_score.append(r2_train)\n",
        "  test_score.append(r2_test)\n",
        "\n",
        "scores = {\n",
        "    \"train_score\": np.array(train_score),\n",
        "    \"test_score\": np.array(test_score)\n",
        "}\n",
        "#anche qui stesso risultato, con train_score sempre a 1, che suggerisce overfitting e test_score piuttosto basso"
      ],
      "metadata": {
        "id": "u0uZ0MT_RPqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*LOOCV*\n",
        "\n",
        "Leave one out cross validation\n",
        "\n",
        "Addestramento su tutto il dataset a eccezione di un unico esempio, utili se abbiamo un dataset con pochi esempi a disposizione.\n",
        "\n",
        "In pratica è come un KFold con numero batch = numero esempi"
      ],
      "metadata": {
        "id": "qUaeXSaJTDvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "loo.get_n_splits(x) # 100 batch\n",
        "\n",
        "train_cost = []\n",
        "test_cost = []\n",
        "\n",
        "#r2 non si può usare col test su un solo esempio, quindi useremo mse\n",
        "for train_index, test_index in loo.split(x):\n",
        "  x_train, x_test = x[train_index], x[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "  #ora possiamo standardizzare...\n",
        "  ss = StandardScaler()\n",
        "  x_train = ss.fit_transform(x_train)\n",
        "  x_test = ss.transform(x_test)\n",
        "\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x_train, y_train)\n",
        "\n",
        "  mse_train = mean_squared_error(y_train, lr.predict(x_train))\n",
        "  mse_test = mean_squared_error(y_test, lr.predict(x_test))\n",
        "\n",
        "  train_cost.append(mse_train)\n",
        "  test_cost.append(mse_test)\n",
        "\n",
        "costs = {\n",
        "    \"train_cost\": np.array(train_cost),\n",
        "    \"test_cost\": np.array(test_cost)\n",
        "}\n",
        "\n",
        "costs[\"train_cost\"].mean() # praticamente zero, indica che è perfetto\n",
        "costs[\"test_cost\"].mean() # valore altissimo, altra conferma di overfitting"
      ],
      "metadata": {
        "id": "EVScJBBeTUFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Modelli di correzione overfitting*"
      ],
      "metadata": {
        "id": "dwla0XlXUc5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=RANDOM_SEED)\n",
        "\n",
        "#standardizziamo i dati\n",
        "ss = StandardScaler()\n",
        "x_train = ss.fit_transform(x_train)\n",
        "x_test = ss.transform(x_test)\n",
        "\n",
        "#modello lineare sempice minimi quadrati\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "evaluate_model(lr, (x_train, y_train)) # overfitting, quindi r2 a 1\n",
        "evaluate_model(lr, (x_test, y_test)) # overfitting, quindi r2 sul test molto basso"
      ],
      "metadata": {
        "id": "aQwqSq0fUheH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Ridge regression (regolarizzazione di tipo 2, meno invasiva)  - L2"
      ],
      "metadata": {
        "id": "h2a8JWo3VH3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge(alpha=1.) #alfa è il labda\n",
        "model.fit(x_train, y_train)\n",
        "evaluate_model(model, (x_train, y_train)) # ancora altissimo r2\n",
        "evaluate_model(model, (x_test, y_test)) # peggiorato rispetto a prima\n",
        "#caso di underfitting, alzando alpha peggiora ancora"
      ],
      "metadata": {
        "id": "f8DW1vsKVM9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  LASSO regression (regolarizzazion di tipo 1) - L1\n",
        "\n",
        "Questa permette di portare il peso di feature ritenute non significative a zero"
      ],
      "metadata": {
        "id": "BDE75StOVtnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "model = Lasso(alpha=1.) #alfa è il labda\n",
        "model.fit(x_train, y_train)\n",
        "evaluate_model(model, (x_train, y_train)) # ancora altissimo r2\n",
        "evaluate_model(model, (x_test, y_test)) # ora r2 rimane molto alto a 0.99, quindi la normalizzazione L1 ha rimosso overfitting\n",
        "#sono state elminate su 100, 90 feature ritenute no valide"
      ],
      "metadata": {
        "id": "23Br058LVxvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Elastic NET\n",
        "\n",
        "Permette di combinare entrambe (L1+L2)"
      ],
      "metadata": {
        "id": "kHN_2aTNWSDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "model = ElasticNet(alpha=1., l1_ratio=1) #alfa è il labda\n",
        "model.fit(x_train, y_train)\n",
        "evaluate_model(model, (x_train, y_train))\n",
        "evaluate_model(model, (x_test, y_test)) # uguale al precente perchè abbiamo messo l1_ratio a 100%"
      ],
      "metadata": {
        "id": "-dXuTW5LWWCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Learing Curve\n",
        "\n",
        "Facciamo curva sul modello migliore identificato, ovvero Lasso."
      ],
      "metadata": {
        "id": "NEV7vXb5Wvjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "train_sizes_abs, train_scores, test_scores = learning_curve(\n",
        "    Lasso(),\n",
        "    x,\n",
        "    y,\n",
        "    random_state = RANDOM_SEED\n",
        ")\n",
        "\n",
        "plt.plot(train_sizes_abs, train_scores.mean(axis=1), label=\"Train score\")\n",
        "plt.plot(train_sizes_abs, test_scores.mean(axis=1), label=\"Test score\")\n",
        "plt.show()\n",
        "\n",
        "#Si parte da soluzione di overfitting, poi aumentando gli esempi si raggiunge un valore perfetto\n"
      ],
      "metadata": {
        "id": "8igCpWbvWxv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regolarizzazione"
      ],
      "metadata": {
        "id": "nhUpcRYyRMsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import LeaveOneOut, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import Ridge,Lasso ,LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/housing.csv\"\n",
        "\n",
        "df = pd.read_csv(DATASET_URL, index_col=0)\n",
        "df.head()\n",
        "\n",
        "x = df.drop(\"PRICE\", axis=1).values # array con feature\n",
        "y = df[\"PRICE\"].values # array con target\n",
        "\n",
        "#creazione array per addetramneto e test\n",
        "x_train, x_test, y_train , y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED)\n",
        "x_train.shape #70% del totale\n",
        "\n",
        "#creazione feature polinomiali di seconod grado\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "x_train = poly.fit_transform(x_train)\n",
        "x_test = poly.transform(x_test)\n",
        "\n",
        "#standardizzazione\n",
        "ss = StandardScaler()\n",
        "x_train = ss.fit_transform(x_train)\n",
        "x_test = ss.transform(x_test)\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "  x, y = dataset\n",
        "  y_pred = model.predict(x)\n",
        "  print(f\"MSE: {mean_squared_error(y, y_pred):.3f}\")\n",
        "  print(f\"R2 {r2_score(y, y_pred):.3f}\")\n",
        "\n",
        "#iniziamo a creare modelli\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "evaluate_model(lr, (x_train, y_train)) #0.95\n",
        "evaluate_model(lr, (x_test, y_test)) #0.63\n",
        "\n",
        "1 - (0.639/0.95) #32% di differenza, l'esercizio chiede sotto al 15...proviamo a ottimizzare\n",
        "\n",
        "model = Ridge(alpha=1.)\n",
        "model.fit(x_train, y_train)\n",
        "evaluate_model(model, (x_train, y_train))\n",
        "evaluate_model(model, (x_test, y_test)) #situazione migliorata, provamiamo regolarizzazione L1 (Lasso)\n",
        "\n",
        "model = Lasso(alpha=1.)\n",
        "model.fit(x_train, y_train)\n",
        "evaluate_model(model, (x_train, y_train))\n",
        "evaluate_model(model, (x_test, y_test)) #peggio\n",
        "\n",
        "# ora usiamo la cross-validation\n",
        "train_score = []\n",
        "test_score = []\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "for train_index, test_index in kf.split(x):\n",
        "  x_train, x_test = x[train_index], x[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "  poly = PolynomialFeatures(degree=2)\n",
        "  x_train = poly.fit_transform(x_train)\n",
        "  x_test = poly.transform(x_test)\n",
        "\n",
        "  ss = StandardScaler()\n",
        "  x_train = ss.fit_transform(x_train)\n",
        "  x_test = ss.transform(x_test)\n",
        "\n",
        "  model = Ridge(alpha=10.)\n",
        "  model.fit(x_train, y_train)\n",
        "\n",
        "  r2_train = r2_score(y_train, model.predict(x_train))\n",
        "  r2_test = r2_score(y_test, model.predict(x_test))\n",
        "\n",
        "  train_score.append(r2_train)\n",
        "  test_score.append(r2_test)\n",
        "\n",
        "scores = {\n",
        "    \"train_score\": np.array(train_score),\n",
        "    \"test_score\": np.array(test_score),\n",
        "}\n",
        "\n",
        "scores[\"train_score\"].mean()\n",
        "scores[\"test_score\"].mean()\n",
        "1 - (scores[\"test_score\"].mean()/scores[\"train_score\"].mean()) #5% di differenza, ottimo risultato\n",
        "\n",
        "#creiamo anche laerning curve del modello migliore\n",
        "sns.set_theme()\n",
        "\n",
        "train_sizes_abs, train_scores, test_scores = learning_curve(\n",
        "    Ridge(alpha=10.),\n",
        "    x,\n",
        "    y,\n",
        "    random_state = RANDOM_SEED\n",
        ")\n",
        "\n",
        "plt.plot(train_sizes_abs, train_scores.mean(axis=1), label=\"Train score\")\n",
        "plt.plot(train_sizes_abs, test_scores.mean(axis=1), label=\"Test score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IqfXraBrRPXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificazione"
      ],
      "metadata": {
        "id": "oZxDA-gUebcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Classificazione binaria*"
      ],
      "metadata": {
        "id": "-nUiaPYcedf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "aGs0-jujialX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 0"
      ],
      "metadata": {
        "id": "c-IftHB0ikUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = make_classification(n_samples = 100,\n",
        "                           n_features = 2,\n",
        "                           n_informative = 2,\n",
        "                           n_redundant = 0,\n",
        "                           n_repeated = 0,\n",
        "                           n_classes = 2, #binaria\n",
        "                           random_state = RANDOM_SEED)\n",
        "\n",
        "x[:,0] #prima feature\n",
        "x[:,1] #seconda feature\n",
        "\n",
        "plt.scatter(x[:,0], x[:,1], c=y) #colora pallini con le categorie\n",
        "#lo scopo è trovare retta che taglia i due gruppi\n",
        "\n",
        "#creaiamo il modello\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "def plot_decision_boundary(model, dataset):\n",
        "  x,y = dataset\n",
        "  h = .02\n",
        "\n",
        "  x_min, x_max = x[:,0].min() -.2, x[:,0].max() + .2\n",
        "  y_min, y_max = x[:,1].min() -.2, x[:,1].max() + .2\n",
        "\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                       np.arange(y_min, y_max, h))\n",
        "\n",
        "  z = model.predict(np.c_[xx.ravel(), yy.ravel() ]) # ravel array multi a single dimension\n",
        "  z = z.reshape(xx.shape)\n",
        "\n",
        "  plt.contourf(xx, yy, z, cmap=plt.cm.Paired)\n",
        "  plt.scatter(x[:,0], x[:,1], c=y, edgecolor='white')\n",
        "\n",
        "plot_decision_boundary(lr, (x_train, y_train)) # quello che divide meglio le classi\n",
        "plot_decision_boundary(lr, (x_test, y_test)) # anche su quello di test\n",
        "\n"
      ],
      "metadata": {
        "id": "afiXVlJGimMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Valutazione del modello*"
      ],
      "metadata": {
        "id": "Siw10hxWmySC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  log loss\n",
        "\n",
        "basata sulla probabilità della regressione logistica, quindi no usiamo predict ma predict_proba"
      ],
      "metadata": {
        "id": "9dj5Zzrfm5Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# tra 0 e 1, probabilità appartenza classe 1 o 2\n",
        "y_proba_train = lr.predict_proba(x_train)\n",
        "y_proba_test = lr.predict_proba(x_test)\n",
        "\n",
        "#log loss uguale a zero indica modello perfetto\n",
        "log_loss(y_train, y_proba_train)\n",
        "log_loss(y_test, y_proba_test)"
      ],
      "metadata": {
        "id": "19b20olSm1No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  matrice di confusione"
      ],
      "metadata": {
        "id": "QYrPCvBmm4ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "y_pred_train = lr.predict(x_train)\n",
        "y_pred_test = lr.predict(x_test)\n",
        "\n",
        "cm = confusion_matrix(y_train, y_pred_train)\n",
        "#matrice TN PF\n",
        "#        FN TP\n",
        "\n",
        "#mostriamo graficamente...\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  df_cm = pd.DataFrame(cm,\n",
        "                        index=[\"Negative\",\"Positive\"],\n",
        "                        columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
        "  sns.heatmap(df_cm, annot=True)\n",
        "\n",
        "plot_confusion_matrix(y_train, y_pred_train)\n",
        "plot_confusion_matrix(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "-DbvNyL1nZgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Accuracy, Precision, Recall, F1\n",
        "\n",
        "score, valori più alti = migliore qualità modello"
      ],
      "metadata": {
        "id": "S7z_xBXXp9Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def classification_report(y_true, y_pred):\n",
        "  print(f\"PRECISION: {precision_score(y_true, y_pred)}\") #%classificazioni positive effettivamente tali\n",
        "  print(f\"RECALL: {recall_score(y_true, y_pred)}\")#%classificazioni positive effettivamente classificate come positive\n",
        "  print(f\"F1: {f1_score(y_true, y_pred)}\") #media aarmonica tra precision e recall\n",
        "  print(f\"ACCURACY: {accuracy_score(y_true, y_pred)}\") #% modello ha categorizzato corretamente\n",
        "\n",
        "classification_report(y_train, y_pred_train)\n",
        "classification_report(y_test, y_pred_test)\n",
        "\n",
        "#in alternativa...\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "7sdLM9tuoZI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  ROC curve"
      ],
      "metadata": {
        "id": "pyj_wzQCsKG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
        "RocCurveDisplay.from_estimator(lr, x_train, y_train) #buon modello, valore alto\n",
        "RocCurveDisplay.from_estimator(lr, x_test, y_test)"
      ],
      "metadata": {
        "id": "YbNYo_DasMXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Classificaizone multi classe"
      ],
      "metadata": {
        "id": "GTDivb0nsNIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba_train = lr.predict_proba(x_train)\n",
        "roc_auc_score(y_train, y_proba_train, multi_class=\"ovo\", average=\"macro\") #oppure ovr o multinomial"
      ],
      "metadata": {
        "id": "2eSrqHWpsQUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificazione (esercizio)"
      ],
      "metadata": {
        "id": "EKeVzENzjKPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "\n",
        "### import del dataset\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/breast_cancer.csv\"\n",
        "df = pd.read_csv(DATASET_URL)\n",
        "df.head()\n",
        "df.count() #563 righe e 32 colonne, con colonna\n",
        "\n",
        "#ID univoco, che è inutile ai fini del modello...\n",
        "df = df.drop(\"ID number\", axis=1)\n",
        "df.head()\n",
        "\n",
        "#dataset sbilanciato verso tumori benigni\n",
        "counts = df[\"diagnosis\"].value_counts()\n",
        "print(f\"Tumori benigni: {counts[0]} / ({counts[0]/counts.sum()*100:.2f}%)\")\n",
        "\n",
        "### codifichiamo il target in numero\n",
        "map_dict = {\"M\":1, \"B\":0}\n",
        "df[\"diagnosis\"] = df[\"diagnosis\"].map(lambda x: map_dict[x])\n",
        "df.head()\n",
        "\n",
        "### creazione del modello...\n",
        "x = df.drop(\"diagnosis\", axis=1) #array features\n",
        "y = df[\"diagnosis\"].values # array target\n",
        "#dividiamo in train e test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=RANDOM_SEED)\n",
        "x_train.shape #70%\n",
        "x_test.shape #70%\n",
        "\n",
        "#standardizzazione..\n",
        "ss = StandardScaler()\n",
        "x_train = ss.fit_transform(x_train)\n",
        "x_test = ss.transform(x_test)\n",
        "\n",
        "#visto che il dataset è sbilanciato, assegniamo peso in base alla proporzione\n",
        "lr = LogisticRegression(class_weight=\"balanced\")\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "## valutazione del modello\n",
        "y_pred_train = lr.predict(x_train)\n",
        "y_proba_train = lr.predict_proba(x_train)\n",
        "y_pred_test = lr.predict(x_test)\n",
        "y_proba_test = lr.predict_proba(x_test)\n",
        "\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test)) #accuracy 0.98, recall invece da requisiti deve essere 1 (nessun falso positivo)\n",
        "# in questo caso potremmo spostare il threshold visto che i positivi hanno più peso dei negativi...\n",
        "y_pred_train = np.where(y_proba_train[:,1]>0.25, 1, 0) #di default è a 0.5\n",
        "y_pred_test = np.where(y_proba_test[:,1]>0.25, 1, 0) #di default è a 0.5\n",
        "print(classification_report(y_train, y_pred_train))\n",
        "print(classification_report(y_test, y_pred_test)) # ora è ok, recall di uno sul dataset di test\n",
        "\n",
        "## grafico confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, labels=[\"Negative\", \"Positive\"], show_precision_recall=False):\n",
        "  cm = confusion_matrix(y_true, y_pred) # tn, fp, fn, tp\n",
        "  df_cm = pd.DataFrame(cm, index=labels,\n",
        "                       columns=[\"Predicated \" + labels[0], \"Predicted \" + labels[1]])\n",
        "  sns.heatmap(df_cm, annot=True, fmt=\"g\")\n",
        "  if show_precision_recall:\n",
        "    plt.text(0,-0.1, f\"Precision: {cm[1][1]/cm[1][1]+cm[0][1]}\") #-0.1 è per miglire visualizzazione\n",
        "    plt.text(0,-0.1, f\"Recall: {cm[1][1]/(cm[1][1]+cm[1][0])}\") #-0.1 è per miglire visualizzazione\n",
        "\n",
        "plot_confusion_matrix(y_train, y_pred_train, show_precision_recall=True)\n",
        "plot_confusion_matrix(y_test, y_pred_test, show_precision_recall=True) #nessun falso positivo come da requisito\n",
        "\n",
        "## roc curve\n",
        "RocCurveDisplay.from_estimator(lr, x_train, y_train, name=\"Cancer Classifier\")\n",
        "\n",
        "## classificazioni sui nuovi dati e salvataggio in file excel\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/breast_cancer_pred.csv\"\n",
        "df_pred = pd.read_csv(DATASET_URL)\n",
        "x = df.drop(\"ID number\", axis=1).values\n",
        "x = ss.transform(x)\n",
        "\n",
        "y_proba = lr.predict_proba(x)\n",
        "y_pred = np.where(y_proba[:,1]>0.25, 1, 0)\n",
        "y_pred\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    \"ID number\": df_pred[\"ID number\"],\n",
        "    \"prediction\": y_pred,\n",
        "    \"probability\": y_proba.max(axis=1).round(4) # la classe con più probabilità\n",
        "})\n",
        "\n",
        "df_results.head()\n",
        "df_results.to_excel(\"breast_cancer_prediction.xlsx\")"
      ],
      "metadata": {
        "id": "bS_Bx9kUjNfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYurWD6xmGD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ],
      "metadata": {
        "id": "1wzZYOFlxhYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*K-means*"
      ],
      "metadata": {
        "id": "PM0OqStdxi31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "y3cnPq6fyjS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
        "sns.set_theme()"
      ],
      "metadata": {
        "id": "_RtW11dOyvIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 2\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "x, _ = make_blobs(n_samples = 100,\n",
        "                  n_features = 2,\n",
        "                  centers = 3,\n",
        "                  cluster_std = 0.5,\n",
        "                  random_state= RANDOM_SEED)\n",
        "\n",
        "# es. 1° feature spesa in birra dei clienti di un market\n",
        "# es. 2° feature spesa in pannolini dei clienti di un market\n",
        "\n",
        "#centriamolo a zero e aumentiamo i valori per avere dei dati di test più sensati\n",
        "x-=x.min(axis=0)\n",
        "x[:,0]*=20\n",
        "x[:,1]*=6"
      ],
      "metadata": {
        "id": "UM8rDc63y4Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=x[:,0],y=x[:,1], s=100) #risultano 3 cluster"
      ],
      "metadata": {
        "id": "fU0X3rLqzUrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creazione del modello\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=RANDOM_SEED) # i centroidi all'inizio sono a caso, non ottimale, potrebbero essere troppo vicini\n",
        "kmeans = KMeans(n_clusters=3, init=\"k-means++\") # in questo modo sono maggiornamente tarati\n",
        "kmeans.fit(x)"
      ],
      "metadata": {
        "id": "kvUdx6Fk0UTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#valutazione del modello (inertia o distotion)\n",
        "kmeans.cluster_centers_ #array con i centroidi\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "#esempio utilizzo...\n",
        "a = np.array([[1,2],[3,2]])\n",
        "b = np.array([[2,1],[3,3]])\n",
        "cdist(a, b, \"euclidean\") # array numpi con distanze tra i punti\n",
        "\n",
        "distortion = sum(np.square(np.min(cdist(x, kmeans.cluster_centers_, 'euclidean'), axis=1)) / x.shape[0]) #107\n",
        "inertia = sum(np.square(np.min(cdist(x, kmeans.cluster_centers_, 'euclidean'), axis=1))) #uguale, senza media\n",
        "#...oppure:\n",
        "kmeans.inertia_"
      ],
      "metadata": {
        "id": "y2levVB61EOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizzazione del cluster\n",
        "y_kmeans = kmeans.predict(x)\n",
        "y_kmeans\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "#aggiungiamo informazioni dedotte dal grafico\n",
        "cluster_map = {0: \"Neo papà\", 1:\"Donne single\", 2:\"Neo mamme\"}\n",
        "vfunc = np.vectorize(lambda x: cluster_map[x])\n",
        "classes = vfunc(y_kmeans)\n",
        "\n",
        "plt.xlabel(\"Spesa in birra\")\n",
        "plt.xlabel(\"Spesa in pannolini\")\n",
        "\n",
        "#mostriamo colori rappresentazioni ottenute, con indicazione dei centroidi\n",
        "sns.scatterplot(x=x[:,0], y=x[:,1], hue=classes, s=100)\n",
        "plt.scatter(centers[:,0], centers[:,1], c=\"red\", s=200, alpha=0.5)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ww5KGHdm2bb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow method, per n valori di k addestrati\n",
        "\n",
        "ssd = {}\n",
        "\n",
        "for k in range(1,10):\n",
        "  kmeans = KMeans(n_clusters=k, init=\"k-means++\").fit(x)\n",
        "  ssd[k] = kmeans.inertia_\n",
        "\n",
        "ssd\n",
        "\n",
        "plt.plot(list(ssd.keys()), list(ssd.values()), marker=\"o\")\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Sum of squared distances (Inertia)')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n",
        "\n",
        "#dal gafico si conferma che il \"gomito\" è a 3"
      ],
      "metadata": {
        "id": "ofpEXtEY4jG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering (esercizio)"
      ],
      "metadata": {
        "id": "LYQ6FtYbD7KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import cdist\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "RANDOM_SEED = 1\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
        "sns.set_theme()\n",
        "\n",
        "DATASET_URL = \"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/mall_customers.csv\"\n",
        "df = pd.read_csv(DATASET_URL)\n",
        "df.head()\n",
        "\n",
        "#funzione per stampare la curva SSD per diversi k\n",
        "def plot_ssd_curve(x, k_range=(1,10)):\n",
        "  ssd = {}\n",
        "  for k in range(k_range[0], k_range[1]):\n",
        "    kmeans = KMeans(n_clusters = k, init=\"k-means++\", random_state=RANDOM_SEED)\n",
        "    kmeans.fit(x)\n",
        "    ssd[k] = kmeans.inertia_\n",
        "\n",
        "  plt.plot(list(ssd.keys()), list(ssd.values()), marker=\"o\")\n",
        "\n",
        "  plt.xlabel(\"Numero di cluster\", fontsize=16)\n",
        "  plt.ylabel(\"Somma della distanza al quadrato\", fontsize=16)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_clusters(model, data, axLabels=None, print_ssd=False):\n",
        "  centers = model.cluster_centers_\n",
        "  y_kmeans = model.predict(data)\n",
        "  sns.scatterplot(x=data[:,0], y=data[:,1], hue=y_kmeans, s=100)\n",
        "  plt.scatter(centers[:,0], centers[:,1], c=\"red\", alpha=.5)\n",
        "\n",
        "  if axLabels!=None:\n",
        "    plt.xlabel(axLabels[0], fontsize=16)\n",
        "    plt.ylabel(axLabels[1], fontsize=16)\n",
        "  if print_ssd:\n",
        "    plt.text(x[:,0].max()-10, 0, f\"SSD={model.inertia_:.2f}\")\n",
        "  plt.show()\n",
        "\n",
        "#modello su age e spending score...\n",
        "x = df[[\"Age\", \"Spending Score (1-100)\"]].values\n",
        "plot_ssd_curve(x) #si vede k=4\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, init=\"k-means++\", random_state=RANDOM_SEED)\n",
        "kmeans.fit(x)\n",
        "kmeans.inertia_\n",
        "\n",
        "plot_clusters(kmeans, x, axLabels=[\"Age\", \"Spending Score\"])\n",
        "\n",
        "#valutiamo altro modello...\n",
        "x = df[[\"Annual Income (k$)\", \"Spending Score (1-100)\"]].values\n",
        "plot_ssd_curve(x) #si vede k=5\n",
        "kmeans = KMeans(n_clusters=5, init=\"k-means++\", random_state=RANDOM_SEED)\n",
        "kmeans.fit(x)\n",
        "kmeans.inertia_ # non si può confrontare con l'altra poichè relativa alla distanza dal centroide e la scala x è diversa, non è standardizzata\n",
        "plot_clusters(kmeans, x, axLabels=[\"Annual Income (k$)\", \"Spending Score (1-100)\"])\n",
        "\n",
        "#valutiamo altro modello con tre variabili, va rappresentato in 3d...\n",
        "x = df[[\"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"]].values\n",
        "plot_ssd_curve(x) #si vede k=6\n",
        "kmeans = KMeans(n_clusters=6, init=\"k-means++\", random_state=RANDOM_SEED)\n",
        "kmeans.fit(x)\n",
        "kmeans.inertia_ # non si può confrontare con l'altra poichè relativa alla distanza dal centroide e la scala x è diversa, non è standardizzata\n",
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "%matplotlib notebook\n",
        "def plot_clusters3(model, data, axLables=None):\n",
        "  centers = model.cluster_centers_\n",
        "  y_kmean = model.predict(x)\n",
        "  ax = plt.axes(projection=\"3d\")\n",
        "  ax.scatter3D(data[:,0], data[:,1], data[:,2], c=y_kmean)\n",
        "  ax.scatter3D(centers[:,0], centers[:,1], centers[:,2], color=\"red\")\n",
        "\n",
        "plot_clusters3(kmeans, x)\n",
        "\n",
        "#utilizziamo il modello\n",
        "df_pred = pd.read_csv(\"https://raw.githubusercontent.com/ProfAI/machine-learning-fondamenti/main/datasets/mall_customers_predict.csv\")\n",
        "df_pred\n",
        "\n",
        "x=df_pred[[\"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"]]\n",
        "x.shape\n",
        "\n",
        "y_kmeans = kmeans.predict(x)\n",
        "y_kmeans #1° osservazione al secondo cluster, ecc\n",
        "\n",
        "df_result = pd.DataFrame({\n",
        "    \"CustomerID\" : df_pred[\"CustomerID\"],\n",
        "    \"Customer Group\": y_kmeans\n",
        "})\n",
        "df_result.head()\n",
        "\n",
        "df_result.to_excel(\"mall_customers_prediction.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-LViTZ_7D97v",
        "outputId": "981f71ab-9c76-4475-abef-5da92cabf8cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   CustomerID  Customer Group\n",
              "0           1               2\n",
              "1           2               1\n",
              "2           3               3\n",
              "3           4               1\n",
              "4           5               2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35f42072-3568-45b8-b3ad-62165f880f5e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Customer Group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35f42072-3568-45b8-b3ad-62165f880f5e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-35f42072-3568-45b8-b3ad-62165f880f5e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-35f42072-3568-45b8-b3ad-62165f880f5e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e5037b41-f0b4-412f-bb87-32cf60ed46f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5037b41-f0b4-412f-bb87-32cf60ed46f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e5037b41-f0b4-412f-bb87-32cf60ed46f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_result",
              "summary": "{\n  \"name\": \"df_result\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"CustomerID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Customer Group\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "hjtbIdIuEZLV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}